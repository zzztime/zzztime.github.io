{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"HDFS YARN   HA的区别","text":"一，hdfs、yarn部署的进程对比 1234hdfs做ha时ruozedata001: ZK NN zkfc jn DNruozedata002: ZK NN zkfc jn DNruozedata003: ZK jn DN 1234yarn做ha时ruozedata001: ZK NN zkfc jn DN RM NMruozedata002: ZK NN zkfc jn DN RM NMruozedata003: ZK jn DN NM 通过上面的比较，yran在做ha时比hdfs多了一个rm 二、HDFS、YARN在做HA的时候架构是不一样的，原因是什么？取决于hdfs和yarn自身的特性不同，hdfs是用来做存储，一个是做计算，数据存储是至关重要的，数据同步或者数据质量有问题，计算指标写的再好也不起作用 三、HDFS中zkfc是进程，而YARN中zkfc是线程,也就是或ps -ef能看到rm，看不到zkfc 四、相同点1、HDFS和YARN都是主从架构master –》slavemaster:nn 、rm slave:dn、nm 为什么dn、nm要部署在同一台机器上dn/nm部署在同一台机器上的原因是数据本地化，计算的时候nm上面有一个container容器做task计算，计算的时候这台机器上有数据，则优先从本地拉取，具体的数据本地化在spark环节会涉及 整个HDFS集群由Namenode和Datanode构成master-worker（主从）模式。Namenode负责构建命名空间，管理文件的元数据等，而Datanode负责实际存储数据，负责读写工作。 2、大数据生态圈 大部分组件都是主从架构提醒：大数据中的hbase组件 进程master和regionserver 坑：hbase读写流程，是不经过master","link":"/2020/03/03/HDFS%20YARN%20HA%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"title":"HDFS小文件的危害","text":"一、black块举例：black块官方设置的是64m,现在多数用的是128m 举例：一个文件130M % 128M=1……………2m结果是1个块128m，1个块2m一碗水130ml 一个瓶子规格容量128ml只能2个瓶子：第1个装满128ml 第二个装不满，实为2ml 二、小文件的危害HDFS 适应场景: 大文件存储，小文件是致命的举例说 明 接上个例子130m的文件10m的10个文件 10块 30m的1个文件 1块 共有11个文件： 11块系统维护一般轻量级会比较好，本身可以2个块去维护，但现在需要11个块维护 ，如果数量级比较大有可能会把nn撑爆 为什么小文件会把NN撑爆？ 例如NN的大小是4G大概是42亿字节1个小文件(阈值&lt;=30m): nn节点维护的字节大约250字节1亿个小文件 250b1亿=250亿字节反而如果1亿个小文件合并100万个大文件：250b1百万=2亿字节则不会被撑爆 如果nn撑爆，在大数据中，大数据的结构基本上都是主从架构，主节点的nn起到非常重要的作用，nn挂了，所有对外的服务，读写的流程将不能运行。生产上:， 1.对小文件阈值进行估算 2.合并小文件，数据未落地到hdfs之前合并或者数据已经落到hdfs，用spark service服务每天调度去合并 -15天 具体情况根据公司的业务周期进行合并，比如今天是15号则合并1号的文件，16号合并2号的 3.小文件危害: 一个是撑爆nn，另一个是hive或者spark计算的时候会影响它的速度","link":"/2020/03/03/HDFS%E5%B0%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%8D%B1%E5%AE%B3/"},{"title":"HDFS常用命令","text":"一、hdfs常用命令fs, fs=hdfs=dfsversion,查看版本jar ,运行一个jar到 yarn上checknative,查看hadoop对于压缩的支持，cloudera提供的cdh默认不支持压缩，得进行源代码编译classpath，hadoop运行的过程中需要加载哪些jar包，假如某一服务的进程无法启动，当 hadoop classpath的时候显示该进程未追加进来，或者额外的自定义的jar、lib包未添加，hadoop classpath会加载一些jar包的路径，第三方的jar包需要额外添加的，将该jar包放在对应的路径。 1234567891011121314151617Usage: hadoop [--config confdir] COMMAND where COMMAND is one of: fs run a generic filesystem user client version print the version jar &lt;jar&gt; run a jar file checknative [-a|-h] check native hadoop and compression libraries availability distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive classpath prints the class path needed to get the credential interact with credential providers Hadoop jar and the required libraries daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settings or CLASSNAME run the class named CLASSNAMEMost commands print help when invoked w/o parameters. （1）checknative，不支持压缩的需要进行编译 12345678910[hadoop@hadoop001 hadoop]$ hadoop checknative19/07/12 18:10:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableNative library checking:hadoop: false zlib: false snappy: false lz4: false bzip2: false openssl: false 19/07/12 18:10:52 INFO util.ExitUtil: Exiting with status 1 （2）classpath 12[hadoop@hadoop001 hadoop]$ hadoop classpath/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/etc/hadoop:/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/*:/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/*:/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs:/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/lib/*:/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/share/hadoop/hdfs/*:/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/lib/*:/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/share/hadoop/yarn/*:/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/lib/*:/home/hadoop/software/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar 二、HDFS的常用命令 123456789[hadoop@hadoop001 ~]$ hdfs dfs -ls[hadoop@hadoop001 ~]$ hdfs dfs -cat[hadoop@hadoop001 ~]$ hdfs dfs chmod -R[hadoop@hadoop001 ~]$ hdfs dfs chown -R[hadoop@hadoop001 ~]$ hdfs dfs -put[hadoop@hadoop001 ~]$ hdfs dfs -mv[hadoop@hadoop001 ~]$ hdfs dfs -text# -format 是属于高危参数# fsck查看文件系统的健康状态 举例说明：从根目录检查，显示状态为健康 123456789101112131415161718192021222324[hadoop@hadoop001 hadoop]$ hdfs fsck /19/07/12 19:56:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableConnecting to namenode via http://hadoop001:50070FSCK started by hadoop (auth:SIMPLE) from /192.168.0.130 for path / at Fri Jul 12 19:56:34 CST 2019..Status: HEALTHY Total size: 54 B Total dirs: 8 Total files: 2 Total symlinks: 0 Total blocks (validated): 2 (avg. block size 27 B) Minimally replicated blocks: 2 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 1 Average block replication: 1.0 Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 1 Number of racks: 1FSCK ended at Fri Jul 12 19:56:34 CST 2019 in 33 millisecondsThe filesystem under path '/' is HEALTHY 如果状态显示非健康，则执行以下操作cd sbin一般启动的是start-dfs.sh在启动的过程中会判断已存在的进程真正底层调用的是hadoop-daemon.sh–&gt;【hadoop-daemon.sh】(说明：根据command调用HDFS。执行相关操作) 12[hadoop@hadoop001 sbin]$ ./hadoop-daemon.sh start datanode# jps——datanode启动了 如果是用shell脚本监控，输出到fsck.log会对日志进行采集判断问题或预警 1[hadoop@hadoop001 ~]$ hdfs fsck / &gt;fsck.log 删除损坏的文件修复损坏的文件请参看块损坏解决方式的文章 1[hadoop@hadoop001 ~]$ hdfs fsck -delete /","link":"/2020/03/03/HDFS%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"title":"HDFS架构及副本放置策略","text":"架构设计一、block块HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M HDFS的Block大的原因是最小化寻道时间。把一个数据块设计的足够大，就能够使得数据传输的时间显著地大于寻找到Block所在时间。这样，传输一个由多个Block组成的文件的时间就取决于磁盘的传输速率实际数据存在datanode的block里，block的映射（即Block Map）在namenode里HDFS中的文件将会按块大小进行分解，并作为独立的单元进行存储 HDFS中一个块只存储一个文件的内容例如：一碗水130ml 一个瓶子规格容量128ml只能2个瓶子：第1个装满128ml 第二个装不满，实为2ml （1）NameNode(NN)NN:名称节点存储: 文件系统的命名空间a.文件名称b.文件目录结构c.文件属性(权限 创建时间 副本数)d.文件对应的哪些块(副本块)–》块对应在哪些DN节点上不会持久化存储这个映射关系，是通过集群的启动和运行时，DataNode定期发送blockReport给NameNode，以此NameNode在【内存】中动态维护这种映射关系作用：管理文件系统的命名空间 （2）DateNode(DN)用来存储数据块和校验数和以及读写数据与NN通信的参数是dfs.heartbeat.interval 3秒 和 dfs.blockreport.intervalMsec 21600000ms=6小时 （3）Secondary NameNode(SNN)作用: 定期合并NN节点的fsimage+editlog为新的fsimage,推送给NN，简称检查点 1234567edits_0000000000000000404-0000000000000000404edits_0000000000000000405-0000000000000000406 拷贝editlogedits_inprogress_0000000000000000407 正在写的日志fsimage_0000000000000000404 拷贝fsimage fsimage_0000000000000000404.md5fsimage_0000000000000000406fsimage_0000000000000000406.md5 123456edits_0000000000000000208-0000000000000000403edits_0000000000000000405-0000000000000000406fsimage_0000000000000000404 包含了过往的所有的editlog001-404fsimage_0000000000000000404.md5fsimage_0000000000000000406fsimage_0000000000000000406.md5 1.滚动新的editlog文件 edits_inprogress_00000000000000004072.将edits_0000000000000000405-0000000000000000406fsimage_0000000000000000404 拷贝到snn节点3.合并为新的image fsimage_00000000000000004064.将检查点的fsimage_0000000000000000406文件推送给nn5.滚动edits_inprogress_0000000000000000407 写满，就滚动到下一个editlog 比如edits_inprogress_0000000000000000408 二、副本数HDFS中副本的容错机制,把块打散分布在不同的节点，生产上副本数dfs.replication一般也是官方默认参数: 3份例如：块大小128m 副本数3份，一个文件260m，请问多少块，多少实际存储？260%128=2…4m 3个块*3个副本=9个块 实际存储260m * 3 =780m 三、副本放置策略第一个副本: 提交节点为DN，自己写一份；否则为集群外提交，则随机挑选一个不太慢、cpu不太忙的节点上第二个副本：放置在于第一个副本的不同机架的节点上第三个副本: 与第二个副本相同机架的不同节点上生产上考量副本数是否需要减少，从两方面一是：存储空间不够，二是：数据量太快太多一般生产上副本数量是3分，历史数据要进行迁移，可迁移至阿里云的oss存储或者AWS的S3存储代码上的命令执行：hdfs dfs -ls hdfs://hadoop001:9000/hdfs dfs -ls oss://hadoop001:9000/hdfs dfs -ls s3://hadoop001:9000/","link":"/2020/03/03/HDFS%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%89%AF%E6%9C%AC%E6%94%BE%E7%BD%AE%E7%AD%96%E7%95%A5/"},{"title":"HDFS HA架构","text":"一、HDFS的HA架构官方站点：http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html 1、HA具备的作用HA指的是：HA(High Available)，高可用性集群，保证业务连续性，实现业务不中断一般有两个或两个以上的节点，且分为活动节点及备用节点。 2、HA是怎么处理运作的HA是使用QJM（Quorum Journal Manager）处理的 ，所谓的HA是nn做HA 如果做集群至少是需要三台机器ZK(ZooKeeper) 可以将节点专门选为活动节点。如果当前活动的NameNode崩溃，则另一个节点可能在ZooKeeper中采用特殊的独占锁，指示它应该成为下一个活动的。ZKFC(ZKFailoverController) 监视和管理NameNode的状态JN(JournalNode)日志集群 例如：ruozedata001: ZK NN zkfc jn DNruozedata002: ZK NN zkfc jn DNruozedata003: ZK jn DN 如果将三台机器的DN放在一台机器上去启动，是没必要不现实的 ，利用阿里云集群去处理如果做成HA的话，HDFS的三个进程中 nn snn dn，snn将变为standby随时对外提供服务 3、通常集群HA部署需要的机器数JN:在行业中至少需要三台 就是2n+1zk:也需要2n+1&lt;=20个节点，zk通常5台就足够了，3台也可以20~100多台的， 通常是7/9/11&gt;100 通常建议是11台如果&gt;100 通常建议是11台 关于zk部署的注意事项一：由于ZooKeeper本身具有轻量级资源要求，因此可以在与HDFS NameNode和备用节点相同的硬件上并置ZooKeeper节点。但是生产生zk往往会和其他进程部署在同一个机器上，这是一个误区如果机器比较少，就把zk进程部署到nn/dn上面进行混合装但是zk是很重要的，如果集群多，购买和划分集群的时候，需要买物理机的时候，可以划分一些配置少一点的，比如说，内存只有48g,core只有8个core这种小轻量的配置，不要统一都是256G内存的服务器，用轻量级的机器专门去部署zk 关于zk注意事项二：1.做HA的时候nn1的active状态的nn挂了，但是standby应当能够选举zk为active状态，如果zk负载比较重，比较繁忙，选举不出来，所以建议单独部署，包括yarn也是如果hdfs无法切，nn切不过来，stady切不出active状态，如果机器有的话需要将zk单独切出来2.不建议zk部署的越来越多，如果越来越多，zk是做选举的，就是说谁做active，谁做standby,机器很多的时候会进行投票选举，投票时间越长对外提供服务的时候是会有问题的所以zk不是越多越好 4、hdfs ha架构流程HA使用active NN, standby NN两个节点解决单点问题。两个NN节点通过JN集群，共享状态，通过ZKFC选举active，监控状态，自动备援。DN会同时向两个NN节点发送心跳 在生产上的问题：结果双写、数据同步双写例如：spark是写到hbase（所有的） + es（保留时间对外 2个月数据 ）也就是说计算结果会写两份一份是先写到es,然后载写到hbase，es根据公司情况定，如果查询3个月则保留3个月数据es查询数据是很快的，如果查询的数据是2个月的则从es中查询，如果是更长时间的则从hbase中查询 数据同步双写 A线(延迟了 半小时) B线(没有延迟 ) 电商节假日 618，日常B线是关闭的 hdfs ha架构具体图示： active nn：接收client的rpc请求并处理，同时自己editlog写一份，也向JN的共享存储上的editlog写一份。也同时接收DN的block report，block location updates 和 heartbeat standby nn:同样会接受到从JN的editlog上读取并执行这些log操作，使自己的NN的元数据和activenn的元数据是同步的，使用说standby是active nn的一个热备。一旦切换为active状态，就能够立即马上对外提供NN角色的服务。也同时接收DN的block report，block location updates 和 heartbeat jn：用于active nn,standby nn的同步数据，本身由一组的JN节点组成的集群，奇数，3台(CDH)，是支持Paxos协议。保证高可用。 ZKFC：监控NN的健康状态向ZK集群定期发送心跳 ，让自己被选举，当自己被ZK选举为主时，zkfc进程通过rpc调用让nn转换为active状态","link":"/2020/03/03/HDFS%20HA%E6%9E%B6%E6%9E%84/"},{"title":"Hive的内部表和外部表","text":"一、内部表和外部表的区别通常生产上最常用的是外部表1、EXTERNAL 12345678910111213141516官方的建表语法：CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) 如果指定了EXTERNAL就是外部表，未指定就是内部表，内部表一删全删，外部表被删数据还在HDFS上面 [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] 数据导入LOAD DATA [LOCAL] INPATH ‘’ [OVERWRITE] INTO TABLE XXX;LOCAL：从本地系统 linux不带LOCAL: 从Hadoop文件系统 HDFS OVERWRITE 数据覆盖不带OVERWRITE 追加 1234官方语法：LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] [INPUTFORMAT 'inputformat' SERDE 'serde'] (3.0 or later) 2、查看表 mysql&gt;select * from TBLS \\G;——可查看到有几个表metadata： TBL_TYPE –&gt;在元数据里面代表表的类型hive&gt;desc formatted ruozedata_emp3; data： Table Type –&gt;在数据hive里面 3、表的类型 表分为两大类：MANAGED_TABLE、EXTERNAL_TABLE MANAGED_TABLE DROP ： data + metadata XEXTERNAL_TABLE DROP: metadata X HDSF √ 举例说明： 12创建一个内部表hive&gt;create table emp_manager as selsct empno,ename.depton from ruozedata_emp; MANAGED_TABLE特性：成功创建后元数据里面也会多一张表HDFS上面有数据，元数据里面有数据hive&gt; drop table emp_managed； ——删除后文件系统不存在，mysql&gt;select * from TBLS \\G;——此时SQL里面表由四条变为三条 1234567891011121314151617181920创建一个外部表官方语法： [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)外部表是需要配一个路径的，默认不放在use，hive，host里面hive&gt;CREATE EXTERNAL TABLE emp_external(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'LOCATION '/d7_externel/emp/' ;创建好之后在HDFS上查看是否存在，/d7_externel——里面内容为空此时在SQL库里面查看，mysql&gt;select * from TBLS \\G;——有表表创建好了，但无数据 123456789外部表导入数据：[hadoop@hadoop001 data]$ hadoop fs -put emp.txt /d7_externel/emp/将员工表拷贝到这个路劲下面hive&gt; select * from emp_external;存在做一个删除操作hive&gt;drop table emp_externel;hive&gt; select * from emp_external;查询不存在打开HDFS 查看仍旧存在mysql&gt;select * from TBLS \\G;不存在","link":"/2020/03/03/HIVE%E7%9A%84%E5%86%85%E9%83%A8%E8%A1%A8%E8%B7%9F%E5%A4%96%E9%83%A8%E8%A1%A8/"},{"title":"Hadoop之Unable to load native-hadoop library for your platform...解决办法","text":"一、出现的问题及原因：配置完hadoop启动的时候出现如下警告信息：无法加载native-hadoop库需要使用适用的builtin-java类 1WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 原因：主要是依赖包 版本 过低的问题，默认Centos6.9的 glibc版本最高为2.12 二、找错误的步骤（1）用ldd命令查看依赖库 12345678[hadoop@hadoop001 native]$ ldd libhadoop.so.1.0.0./libhadoop.so.1.0.0: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by ./libhadoop.so.1.0.0) linux-vdso.so.1 =&gt; (0x00007fff615ca000) libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f70933c4000) libjvm.so =&gt; not found libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f709302f000) /lib64/ld-linux-x86-64.so.2 (0x00000033d2400000)可以看到依赖的都是/lib64/的动态库，version `GLIBC_2.14' 未找到 （2）由上一步可以看出glibc_2.14未找到，所以检查系统的glibc库, ldd –version即可检查。 1234567[hadoop@hadoop001 native]$ ldd --versionldd (GNU libc) 2.12Copyright (C) 2010 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.Written by Roland McGrath and Ulrich Drepper.原来系统预装的glibc库是2.12版本，而hadoop期望是2.14版本 （3）确认当前的系统预装gilbc库的版本使用命令：strings /lib64/libc.so.6|grep GLIBC 12345678910111213141516GLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_2.6GLIBC_2.7GLIBC_2.8GLIBC_2.9GLIBC_2.10GLIBC_2.11GLIBC_2.12GLIBC_PRIVATE 三、解决方案升级glibc 库 （1）下载并解压 12345[root@dsa ~]# cd /usr/local/src/[root@dsa src]# wget http://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz[root@dsa src]# tar xf glibc-2.17.tar.gz [root@dsa src]# cd glibc-2.17 （2）编译编译环节需要注意：编译定义目录和源码目录区分开，否则会导致编译错误 123456[root@dsa glibc-2.17]# mkdir build; cd build[root@dsa build]# ../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin[root@dsa build]# make -j 4[root@dsa build]# make localedata/install-locales[root@dsa build]# make installmake localedata/install-locales一定要执行 执行完make install之后有这一句是证明安装成功了：Your new glibc installation seems to be ok. 12345错误示例：未将编译定义目录和源码目录区分开/usr/bin/install: `include/limits.h' and `/usr/local/glibc-2.14/include/limits.h' are the same filemake[1]: *** [/usr/local/glibc-2.14/include/limits.h] Error 1make[1]: Leaving directory `/usr/local/glibc-2.14'make: *** [install] Error 2 （3）一定要校验版本是否支持 12345678910111213141516171819202122[root@hadoop001 build]# strings /lib64/libc.so.6 | grep GLIBCGLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_2.6GLIBC_2.7GLIBC_2.8GLIBC_2.9GLIBC_2.10GLIBC_2.11GLIBC_2.12GLIBC_2.13GLIBC_2.14GLIBC_2.15GLIBC_2.16GLIBC_2.17GLIBC_PRIVATE 四、完成升级 12345678910[hadoop@hadoop001 ~]$ hadoop checknative19/08/08 23:30:26 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native19/08/08 23:30:26 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib libraryNative library checking:hadoop: true /home/hadoop/software/hadoop-2.6.0-cdh5.15.1/lib/native/libhadoop.so.1.0.0zlib: true /lib64/libz.so.1snappy: true /usr/lib64/libsnappy.so.1lz4: true revision:10301bzip2: true /lib64/libbz2.so.1openssl: true /usr/lib64/libcrypto.so 参考博客：https://blog.csdn.net/u010003835/article/details/81127984","link":"/2020/03/03/Hadoop%E4%B9%8BUnable%20to%20load%20native-hadoop%20library%20for%20your%20platform%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"},{"title":"Hadoop伪分布式部署","text":"一、使用软件 Hadoop版本:hadoop-2.6.0-cdh5.7.0 JDK版本：jdk-8u45-linux-x64 Linux安装ssh服务 二、官方链接 CDH(www.cloudera.com) http://archive.cloudera.com/cdh5/cdh/5/ http://www.apache.org/ http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2-changes.log#慎用CDH5.11.0 三、伪分布式Hadoop部署 1、wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz或 rz上传hadoop文件 2、创建专门维护的用户 hadoop 3、JDK部署 #jdk的目录必须放在/usr/java#如文件夹未创建mkdir -p /usr/java#JDK解压的时候用户和用户组发生变更 必须修正#修改用户和用户组的命令：chown -R root:root /usr/java/jdk1.8.0_45#配置全局环境变量 vi /etc/profile进入vi模式末行输入JAVA HOME=/usr/java/jdk1.8.0_45/ 4、创建4个文件夹software，app，data，log移动hadoop文件到software文件夹下面，改变hadoop用户和用户组 123[hadoop@hadoop001 ~]$ pwd/home/hadoop[hadoop@hadoop001 ~]$ mkdir app software data log 1234567[root@hadoop001]# mv hadoop-2.6.0-cdh...gz /home/hadoop/software/[root@hadoop001]# cd /home/hadoop/software/[hadoop@hadoop001 software]$ chown hadoop:hadoop /home/hadoop/software/[hadoop@hadoop001 software]$ lltotal 304292drwxr-xr-x. 15 hadoop hadoop 4096 Jul 2 11:48 hadoop-2.6.0-cdh5.7.0-rw-r--r--. 1 hadoop hadoop 311585484 Jul 2 15:24 hadoop-2.6.0-cdh5.7.0.tar.gz 5、解压并做软连接，将软连接放在app目录下 12345[hadoop@hadoop001 software]$ tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz[hadoop@hadoop001 software]$ ln -s /home/hadoop/software/hadoop-2.6.0-cdh5.7.0 /home/hadoop/app/hadoop[hadoop@hadoop001 software]$ cd ../app[hadoop@hadoop001 app]$ lllrwxrwxrwx. 1 hadoop hadoop 43 Jul 2 10:36 hadoop -&gt; /home/hadoop/software/hadoop-2.6.0-cdh5.7.0 6、（1）vi hadoop-env.sh#The java implementation to use.export JAVA_HOME=/usr/java/jdk1.8.0_45 （2）vi core-site.xml不得删除第一行和第二行内容，第一行内容和第二行内容是标识是xml格式 1234567#补充&lt;configuration&gt;里面的内容&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （3）vi hdfs-site.xml 1234567#补充&lt;configuration&gt;里面的内容&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7、ssh免密码设置 123456[hadoop@hadoop001 ~]$ ssh-keygen[hadoop@hadoop001 ~]$ cd ssh[hadoop@hadoop001 .ssh]$ cat id_rsa.pub &gt;&gt; authorized_keys[hadoop@hadoop001 .ssh]$ chmod 600 authorized_keys[hadoop@hadoop001 .ssh]$ ssh localhost dateSun Jun 30 21:36:58 CST 2019 8、格式化namenode由于linux是linux文件系统，HDFS相当于文件系统，不认可，有自己的文件系统格式（successfully formatted） 123[hadoop@hadoop001 hadoop]$ pwd/home/hadoop/app/hadoop[hadoop@hadoop001 hadoop] bin/hdfs namenode -format 9、启动hadoop 12345[hadoop@hadoop001 hadoop]$ sbin/start-dfs.sh[root@hadoop001 jdk1.8.0_45]# netstat -nlp|grep 50070tcp 0 0 0.0.0.0:50070 0.0.0.0:* LISTEN 5640/java [root@hadoop001 jdk1.8.0_45]# #通过50070端口，浏览器访问http://服务器IP:50070访问 或者 123456[hadoop@hadoop001 sbin]$ jps4887 DataNode5208 Jps4795 NameNode5100 SecondaryNameNode[hadoop@hadoop001 sbin]$","link":"/2020/03/03/Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/"},{"title":"Hadoop用户启动hdfs三个进程","text":"Hadoo伪分布式部署中需要设置NN、DN、SNN的相关参数，在真正生产中，不可能使用IP来启动相关组件得，因为IP很有可能会变动，也很可能你得修改大量代码中得IP。所以需要调整NN、DN、SNN启动方式localhost为hadoop001 一、先查看当前服务是以localhost启动的 123456789101112131415161718192021222324[root@hadoop001 ~]# su - hadoop[hadoop@hadoop001 ~]$ cd app/[hadoop@hadoop001 app]$ cd hadoop/[hadoop@hadoop001 hadoop]$ sbin/start-dfs.sh 19/07/04 20:57:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [localhost]localhost: starting namenode, logging to /home/hadoop/software/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.outlocalhost: starting datanode, logging to /home/hadoop/software/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /home/hadoop/software/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop001.out19/07/04 20:57:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[hadoop@hadoop001 hadoop]$ jps3778 SecondaryNameNode3500 NameNode3597 DataNode3903 Jps[hadoop@hadoop001 hadoop]$ netstat -nlp|grep 3500#如果用另外一个服务的账号去查看端口号会抛错#如果查看进程没有端口号，尝试用root查看(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)tcp 0 0 0.0.0.0:50070 0.0.0.0:* LISTEN 3500/java tcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTEN 3500/java tcp 0 0 :::35001 :::* LISTEN - 企业里不可在配置文件里配置ip，而以hostname配置 hosts事先配置好ip必须变更hosts文件: 第一二行 千万不要删除注释掉；配置内网ip hostname 二.NN、DN、SNN参数配置(1) NN参数配置 1234567core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; (2) DN参数配置 12345[hadoop@hadoop001 hadoop]$ cat slaves localhost[hadoop@hadoop001 hadoop]$ vi slaves [hadoop@hadoop001 hadoop]$ cat slaves hadoop001 (3) SNN参数配置 1234567891011121314151617[hadoop@hadoop001 hadoop]$ vi hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop001:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop001:50091&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 三.用hadoop用户启动hdfs进程先把进程停止然后再启动此时hdfs进程的启动显示全部为hadoop-01的主机名 123456789[hadoop@hadoop001 hadoop]$ sbin/start-dfs.sh19/07/04 22:02:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [hadoop001]hadoop001: starting namenode, logging to /home/hadoop/software/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.outhadoop001: starting datanode, logging to /home/hadoop/software/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.outStarting secondary namenodes [hadoop001]hadoop001: starting secondarynamenode, logging to /home/hadoop/software/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop001.out19/07/04 22:02:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[hadoop@hadoop001 hadoop]$","link":"/2020/03/03/Hadoop%E7%94%A8%E6%88%B7%E5%90%AF%E5%8A%A8hdfs%E4%B8%89%E4%B8%AA%E8%BF%9B%E7%A8%8B/"},{"title":"Hadoop 集群部署","text":"一、准备环境1、准备3台机器2、安装必要软件包 准备三台2C 8G的CentOS7.2的服务器，设置三台主机名分别为ruozedata001，ruozedata002，ruozedata003|组件版本|百度网盘链接 ||–|–|| jdk-8u45-linux-x64.gz |https://pan.baidu.com/s/1-9CDzxih8nOgCTu__rZb2Q 提取码：wfjk ||zookeeper-3.4.6.tar.gz| https://pan.baidu.com/s/1bhAdTa9HxmxtVrupzZllig 提取码：rbwn ||hadoop-2.6.0-cdh5.15.1.tar.gz| https://pan.baidu.com/s/1sONPvGgGzE2OCY_Q92ix3Q 提取码：ssq8| 二、环境安装1、添加用户和文件夹并上传软件 1234useradd hadoopsu - hadoopmkdir app software source data lib script tmp maven_repos在software文件目录下上传组键并设置软链接 2、三台服务器绑定内网ip跟hostname 123456[root@ruozedata001~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6172.16.69.173 ruozedata001172.16.69.174 ruozedata002172.16.69.172 ruozedata003 3、主机之间配置ssh信任关系 123456789[hadoop@ruozedata001 ~]$ ssh-keygen[hadoop@ruozedata002 .ssh]$ sz id_rsa.pub有时候拷贝文件会换行获取拷贝文件不全，所以下载到本地然后在上传[hadoop@ruozedata001 ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys将ruozedata002跟ruozedata003的公钥文件上传到ruozedata001上[hadoop@ruozedata001 ~]$ cat ~/.ssh/id_rsa_2.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@ruozedata001 ~]$ cat ~/.ssh/id_rsa_3.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@ruozedata001 ~]$ chmod 0600 ~/.ssh/authorized_keys设置authorized_keys的权限 三、组件安装1、安装JDK 1234567891011[root@ruozedata001 ~]# mkdir /usr/java/[root@ruozedata001 ~]# cd /usr/java/上传或者下载jdk文件，解压jdk压缩包的时候要注意所属组跟所属用户的相关权限配置/etc/profile环境变量export JAVA_HOME=/usr/java/jdk1.8.0_45export PATH=$JAVA_HOME/bin/:$PATH[root@ruozedata001 java]# source /etc/profile[root@ruozedata001 java]# java -versionjava version &quot;1.8.0_45&quot;Java(TM) SE Runtime Environment (build 1.8.0_45-b14)Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode) 2、zookeeper的安装 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[hadoop@ruozedata001 ~]$ tar -zxvf /home/hadoop/software/zookeeper-3.4.6.tar.gz -C /home/hadoop/app/zookeeper设置zookeeper环境变量export ZOOKEEPER_HOME=/home/hadoop/app/zookeeperexport PATH=$ZOOKEEPER_HOME/bin/:$PATH[hadoop@ruozedata001 ~]# source ~/bash_profile配置文件的修改[hadoop@ruozedata001 conf]$ cp zoo_sample.cfg zoo.cfg[hadoop@ruozedata001 conf]$ cat zoo.cfg# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/home/hadoop/data/zookeeper 存放zookeeper的数据存储目录# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.#各个服务器之间的通信server.1=ruozedata001:2888:3888server.2=ruozedata002:2888:3888server.3=ruozedata003:2888:3888[hadoop@ruozedata001 conf]$ mkdir /home/hadoop/data/zookeeperzookeeper集群需要有id的标识，按顺序第一台为1第二台为2第三台为3[hadoop@ruozedata001 conf]$ echo 1 &gt; /home/hadoop/data/zookeeper/myid[hadoop@ruozedata002 conf]$ echo 2 &gt; /home/hadoop/data/zookeeper/myid[hadoop@ruozedata003 conf]$ echo 3 &gt; /home/hadoop/data/zookeeper/myid特别需要注意： &gt; 左右需要留有空格[hadoop@ruozedata001 ~]$ vi .bash_profileexport ZOOKEEPER_HOME=/home/hadoop/app/zookeeper[hadoop@ruozedata001 ~]$ source .bash_profile[hadoop@ruozedata001 ~]$ which zkServer.sh~/app/zookeeper/bin/zkServer.sh启动zookeeper[hadoop@ruozedata001 ~]$ zkServer.sh start[hadoop@ruozedata001 ~]$ zkServer.sh status 3、hadoop安装 123456789[hadoop@ruozedata001 ~]$ tar -zxvf /home/hadoop/software/hadoop-2.6.0-cdh5.15.1.tar.gz -C /home/hadoop/app/hadoop设置hadoop环境变量export HADOOP_HOME=/home/hadoop/app/hadoopexport PATH=$HADOOP_HOME/bin/:$HADOOP_HOME/sbin:$PATH[hadoop@ruozedata001 ~]$ source ~/bash_profile修改hadoop的相关配置文件core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves 1234启动JournalNode进程[hadoop@ruozedata001 ~]$ hadoop-daemon.sh start journalnode格式化一台namenode[hadoop@ruozedata001 ~]$ hadoop namenode -format 12345678将第一台格式化的namenode文件夹直接放置到第二台第三台scp -r name ruozedata002:/home/hadoop/data/dfsscp -r name ruozedata003:/home/hadoop/data/dfs[hadoop@ruozedata001 dfs]$ lltotal 12drwx------ 3 hadoop hadoop 4096 Aug 21 13:51 datadrwxrwxr-x 3 hadoop hadoop 4096 Aug 21 12:00 jndrwxrwxr-x 3 hadoop hadoop 4096 Aug 21 13:51 name 12345678910初始化zkfchdfs zkfc -formatZK19/08/21 12:31:44 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/ruozeclusterg7 in ZK.19/08/21 12:31:44 INFO zookeeper.ZooKeeper: Session: 0x26cb23f5d600000 closed19/08/21 12:31:44 INFO zookeeper.ClientCnxn: EventThread shut down19/08/21 12:31:44 INFO tools.DFSZKFailoverController: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down DFSZKFailoverController at ruozedata001/172.18.54.52************************************************************/ 1234567891011121314151617181920212223242526272829303132启动dfsstart-dfs.shStarting namenodes on [ruozedata001 ruozedata002]ruozedata002: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.15.1/logs/hadoop-hadoop-namenode-ruozedata002.outruozedata001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.15.1/logs/hadoop-hadoop-namenode-ruozedata001.out: Name or service not knownstname ruozedata002: Name or service not knownstname ruozedata001错误原因：linux文件分为两种1是win上传的格式，还有linux本身的格式 解决方式：切换为root用户which dos2unixyum install -y dos2unix回到hadoop用户下面cd app/hadoop/etc/hadoop/dos2unix slaves 转换文件格式为Unix[hadoop@ruozedata001 hadoop]$ file slavesslaves: ASCII text将转换后的文件放置到第二台和第三台机器上，并再次启动dfs验证[hadoop@ruozedata001 hadoop]$ scp slaves ruozedata002:/home/hadoop/app/hadoop/etc/hadoopslaves 100% 38 0.0KB/s 00:00 [hadoop@ruozedata001 hadoop]$ scp slaves ruozedata003:/home/hadoop/app/hadoop/etc/hadoopslaves 100% 38 0.0KB/s 00:00 [hadoop@ruozedata001 hadoop]$ start-dfs.shjps查看启动的进程[hadoop@ruozedata001 hadoop]$ jps4065 Jps2436 QuorumPeerMain3767 DataNode3049 NameNode2732 JournalNode3358 DFSZKFailoverController 1234567YARN启动,但ruozedata002需要自己手动启动[hadoop@ruozedata001 ~]$ start-yarn.sh[hadoop@ruozedata002 ~]$ yarn-daemon.sh start resourcemanagerhadoop002: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop002.outhadoop003: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop003.outhadoop001: starting nodemanager, logging to /opt/software/hadoop/logs/yarn-rootnodemanager-hadoop001.out注意：yarn的端口号地址访问，resourcemanager（Active）可以用8088直接访问，resourcemanager（Stadby）需要在8088/cluster/cluster 12启动jobhistory[hadoop@ruozedata002 ~]$ mr-jobhistory-daemon.sh start historyserver hadoop集群已经部署完成","link":"/2020/03/03/Hadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"title":"Hive部署","text":"介绍——官网：hive.apache.org （1）hive介绍Apache Hive数据仓库有助于使用SQL读取，编写和管理驻留在分布式存储中的大型数据集，SQL来完成大数据的统计分析目前市面上使用多的分布式存储 distributed storage：HDFS S3 OSS COS hdfs://hadoop000:8020/xxxxx s3a://…. s3n:// 访问Hive的方式：A command line tool and JDBC driver （2）hive作用Apache社区的顶级项目 Hive：facebook 解决海量的结构化日志的统计问题 刚开始时是作为Hadoop项目的一个子项目的，后面才单独成为一个项目 Hive是构建在Hadoop之上的数据仓库 适合处理离线 Hive是一个客户端，不是一个集群，把SQL提交到Hadoop集群上去运行 Hive是一个类SQL的框架， HQL和SQL的关系 Hive职责：SQL ==&gt; MR/Spark Hive底层支持的引擎：MR/Spark/Tez ***** 统一的元数据管理常用框架， 可互相关联，是由于基于元数据管理 SQL on Hadoop : Spark SQL/Hive/Impala/Presto 一、部署架构元数据：描述数据的数据 例如mysql里面存很多行多列的数据，这张表里面表名、表字段类型 Hive数据分为两部分HDFS + 元数据(MySQL) people.txt + people(id:int,name:string,age:int) （1）为什么能用SQL来进行大数据统计分析？因为有元数据的支撑，我们知道HDFS上的数据每一列字段名是什么，字段类型是什么，数据在HDFS的什么位置 （2）mysql需要主备 二：Hive部署1）下载：http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.15.1.tar.gz2）解压： tar -zvxf hive-1.1.0-cdh5.15.1.tar.gz -C /app/3）将HIVE_HOME配置到系统环境变量(/.bash_profile)export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.15.1export PATH=$HIVE_HOME/bin:$PATH4）将MySQL驱动拷贝到$HIVE_HOME/lib5）安装MySQL6）修改hive的配置文件 hive-site.xml7）启动hive 1234567#将下载好的Hive包下载到software文件夹并解压[hadoop@hadoop001 software]$ lltotal 429956drwxr-xr-x. 15 hadoop hadoop 4096 Jul 6 17:59 hadoop-2.6.0-cdh5.7.0-rw-r--r--. 1 hadoop hadoop 311585484 Jul 2 15:24 hadoop-2.6.0-cdh5.7.0.tar.gzdrwxr-xr-x. 11 hadoop hadoop 4096 Aug 9 2018 hive-1.1.0-cdh5.15.1-rw-r--r--. 1 hadoop hadoop 128670894 Jul 16 13:25 hive-1.1.0-cdh5.15.1.tar.gz 1234567891011#将解压文件创建软连接到app目录下[hadoop@hadoop001 ~]$ cd /home/hadoop/software[hadoop@hadoop001 software]$ ln -s /home/hadoop/software/hive-1.1.0-cdh5.15.1 /home/hadoop/app/hive[hadoop@hadoop001 software]$ cd[hadoop@hadoop001 ~]$ cd /home/hadoop/app/[hadoop@hadoop001 app]$ lltotal 0lrwxrwxrwx. 1 hadoop hadoop 43 Jul 2 10:36 hadoop -&gt; /home/hadoop/software/hadoop-2.6.0-cdh5.7.0lrwxrwxrwx. 1 hadoop hadoop 42 Jul 16 13:52 hive -&gt; /home/hadoop/software/hive-1.1.0-cdh5.15.1[hadoop@hadoop001 app]$ cd hive[hadoop@hadoop001 hive]$ ll 123456#配置Hive的环境变量vi ~/.bash_profile[hadoop@hadoop001 hive]$ cat ~/.bash_profile...export HIVE_HOME=/home/hadoop/app/hiveexport PATH=${HIVE_HOME}/bin:$PATH... 1234#官网下载MySQL官网驱动#解压后将驱动拷贝到$HIVE_HOME/lib[hadoop@hadoop001 lib]$ ll mysql*-rw-r--r--. 1 hadoop hadoop 872303 Jul 16 19:08 mysql-connector-java-5.1.27-bin.jar 12345678910111213141516171819202122232425262728293031323334353637#Hive没有hive-site.xml文件，可以将hdfs-site.xml文件cp至hive的conf目录下#将cp的文件重新命名，并调整参数&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop001:3306/ruozedata_d7?createDatabaseIfNotExist=true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.cli.print.header&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 12345678#启动Hive[hadoop@hadoop001 ~]$ hivewhich: no hbase in (/home/hadoop/app/hive/bin:/home/hadoop/app/hadoop/bin:/home/hadoop/app/hadoop/sbin:/usr/local/mysql/bin:/usr/java/jdk1.8.0_45/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin)19/07/16 22:52:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableLogging initialized using configuration in jar:file:/home/hadoop/software/hive-1.1.0-cdh5.15.1/lib/hive-common-1.1.0-cdh5.15.1.jar!/hive-log4j.propertiesWARNING: Hive CLI is deprecated and migration to Beeline is recommended.hive&gt; 完成Hive的部署 三、Hive1、（1）Hive的文件位置默认的Hive数据存放在HDFS上：/user/hive/warehouse/user/hive/warehouse这个路径是有参数是可以进行控制的，Hive数据存放的HDFS的路径，如果要调整，就把这个参数设置到hive-site.xml中去 1234&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;xxxx&lt;/value&gt; &lt;/property&gt; default数据库对应的目录是hive.metastore.warehouse.dir所以创建一张表，Hive中的表其实对应的就是HDFS上的一个目录，默认文件的名字就是tablename （2）Hive的信息配置hive的信息是可以配置在hive-site.xml里面 全局同时命令行中也能设置 当前session 第一种方式：以set hive.cli.print.current.db为例，在命令行里面设置用set,可调整 = set 参数； 查看当前参数的值 set 参数=值; 真正的设置参数对应的值 第二种方式：了解 hive –hiveconf k=v –hiveconf k=v （3）Hive的存储方式：Hive中的数据在HDFS上都是以文件夹/文件的方式存储的database table partition bucket注：location默认的是在hdfs上use/werehost上面，自己指定的是不带DB的，需要创建例如：create table xxx(id int,name string) （4）删除：DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];DROP DATABASE IF EXISTS d7_hive2 CASCADE;数据库删除的时候，只要库下有表，默认就不能删除CASCADE慎用 2、SQL：DDL DMLDDL:数据定义语言，例如创建修改删除DDL：create delete alterDML: SQL每一个SQL语句底层都有相应的元数据信息做关联在Hive里面创建的表在mysql中查看 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071mysql&gt; use ruozedata_d7Database changedmysql&gt; show tables;+---------------------------+| Tables_in_ruozedata_d7 |+---------------------------+| bucketing_cols || cds || columns_v2 || database_params || dbs || func_ru || funcs || global_privs || part_col_stats || partition_key_vals || partition_keys || partition_params || partitions || roles || sd_params || sds || sequence_table || serde_params || serdes || skewed_col_names || skewed_col_value_loc_map || skewed_string_list || skewed_string_list_values || skewed_values || sort_cols || tab_col_stats || table_params || tbls || version |+---------------------------+29 rows in set (0.00 sec)查看数据库在HDFS对应的文件路径mysql&gt; select * from dbs;+-------+-----------------------+-------------------------------------------+---------+------------+------------+| DB_ID | DESC | DB_LOCATION_URI | NAME | OWNER_NAME | OWNER_TYPE |+-------+-----------------------+-------------------------------------------+---------+------------+------------+| 1 | Default Hive database | hdfs://hadoop001:9000/user/hive/warehouse | default | public | ROLE |+-------+-----------------------+-------------------------------------------+---------+------------+------------+1 row in set (0.00 sec)查看HIVE版本信息mysql&gt; select * from version;+--------+-----------------+-------------------+-----------------------------------+| VER_ID | SCHEMA_VERSION | SCHEMA_VERSION_V2 | VERSION_COMMENT |+--------+-----------------+-------------------+-----------------------------------+| 1 | 1.1.0-cdh5.15.1 | NULL | Set by MetaStore hadoop@10.9.0.62 |+--------+-----------------+-------------------+-----------------------------------+1 row in set (0.00 sec)查看对应的表数据mysql&gt; select * from tbls;+--------+-------------+-------+------------------+--------+-----------+-------+-----------+---------------+--------------------+--------------------+| TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER | RETENTION | SD_ID | TBL_NAME | TBL_TYPE | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT |+--------+-------------+-------+------------------+--------+-----------+-------+-----------+---------------+--------------------+--------------------+| 1 | 1563267428 | 1 | 0 | hadoop | 0 | 1 | ruozedata | MANAGED_TABLE | NULL | NULL |+--------+-------------+-------+------------------+--------+-----------+-------+-----------+---------------+--------------------+--------------------+1 row in set (0.00 sec)查看对应表的字段信息mysql&gt; select * from columns_v2;+-------+---------+-------------+-----------+-------------+| CD_ID | COMMENT | COLUMN_NAME | TYPE_NAME | INTEGER_IDX |+-------+---------+-------------+-----------+-------------+| 1 | NULL | age | int | 2 || 1 | NULL | id | int | 0 || 1 | NULL | name | string | 1 |+-------+---------+-------------+-----------+-------------+3 rows in set (0.00 sec)","link":"/2020/03/03/Hive%E7%9A%84%E9%83%A8%E7%BD%B2/"},{"title":"Linux中常见的错误与校验方法","text":"1.常见的错误有四种| 错误提示 | 说明||–|–|| ping: unknown host |主机找不到错误 || timeout | 请求超时 ||Connection refused|连接拒绝||permission denied | 权限被拒绝 | 2.常见四种错误的解决方式（1）当提示ping: unknown host时，原因是映射关系未配置导致，也就是机器的名称对应的IP未进行配置Linux配置方式：vi /etc/hosts 输入IP地址和机器名称window配置方式: C:\\Windows\\System32\\drivers\\etc\\hosts将hosts文件直接复制一份桌面上，修改后进行覆盖即可 （2）当提示timeout 时， 用ping ip/hostname （3）当提示Connection refused时， 用telnet ip port telnet ip port window位置在控制面板-程序和功能-启动或关闭window功能-勾选Telnetlinux 先安装 yum install -y telnet然后在telnet端口号和IP 123456[root@hadoop001 ~]# which telnet/usr/bin/telnet[root@hadoop001 ~]# telnet 192.168.0.130 80 Trying 192.168.0.130...Connected to 192.168.0.130.Escape character is '^]'. （4）当提示permission denied时， 利用sudo命令获取root权限vi /etc/sudoers进入vi编辑模式，按i键在尾行提示– INSERT – W10: Warning: Changing a readonly file光标移动至#Allow root to run any commands anywhereroot ALL=(ALL) ALL输入Wendy ALL=(root) NOPASSWD:ALL:wq提示E45: ‘readonly’ option is set (add ! to override)重新输入:wq!","link":"/2020/03/02/Linux%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E9%94%99%E8%AF%AF%E4%B8%8E%E6%A0%A1%E9%AA%8C%E6%96%B9%E6%B3%95/"},{"title":"Linux基础命令02","text":"vi文件时拷贝内容必须为INSERT模式，防止丢失粘贴的部分文字 ———————————————————————1.别名alias设置别名时为临时，只是当前session生效，并未进行全局设置2环境变量全局环境变量 /etc/profile 意味着所有人可以使用用户环境变量 ~/.bash_profile 3.删库rm Wendy.log 文件 询问rm -f tail1.log 文件 不询问rm -rf 6 文件夹 不询问rm -rf /生产上是致命的规避: 生产上凡是碰见rm -rf强制删除文件夹的 ，路径一定先判断是否存在，不存在就skip，存在就rm，删除前要仔细确认4.逃避责任vpn 不会记录操作记录堡垒机 记录命令操作记录查看历史记录：historymore .bash_history清空文件cat /dev/null &gt; .bash_history命令清空 history -c 同时删除家目录的.bash_history5.history!5表示执行的第五行删除任务当拿到已经存在的工作集群，通过history查看当前的集群环境，查看之前的历史操作 6.用户和用户组的常用命令ll /usr/sbin/user* 模糊匹配,对符合的都进行打印显示一个用户可以在多个用户组 但是必须有一个主组uid=501(Wendy1) gid=501(Wendy1) groups=501(Wendy1) 用户名 主组 所有组 7.删除用户userdel Wendy1 删除用户会把passwd记录删除，同时假如该组没有其他用户则删除该组[root@hadoop001 ~]# userdel Wendy1 [root@hadoop001 ~]# cat /etc/passwd 8.创建用户useradd zhangsan[root@hadoop001 ~]# useradd zhangsan useradd: warning: the home directory already exists. Not copying any file from skel directory into it. Creating mailbox file: File exists #家目录已经存在不需要再次创建也不需要拷贝skel目录的文件，skel directory指的是 .bash* 所有的隐藏文件 .bash_profile .bashrc 这两个文件会影响命令进入的命令格式,-其中一种是bash-4.1# [Wendy@hadoop001 ~]$))) 9.用户组命令groupadd 添加用户组添加用户组新成员Wendy1 usermod -a -G bigdata Wendy1调整Wendy1的主组为bigdata 调整Wendy1的主组为bigdata","link":"/2020/03/02/Linux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A402/"},{"title":"Linux基础命令03","text":"1.权限指令sudo允许系统管理员让普通用户执行一些或者全部的root命令 2.进程和端口号ps -ef将后台所有的进程打印出来ps -ef|grep tail将后台进程中过滤的关键词tail进程打印grep -v过滤排除 12345678910[root@hadoop001 ~]# ps -ef|grep tailroot 3140 2981 0 02:05 pts/1 00:00:00 tail -F tail.logroot 3144 2919 0 02:06 pts/0 00:00:00 grep tail[root@hadoop001 ~]# ps -ef|grep tail | grep -v greproot 3140 2981 0 02:05 pts/1 00:00:00 tail -F tail.log[root@hadoop001 ~]# ps -ef|grep 2981root 2981 2977 0 01:28 pts/1 00:00:00 -bashroot 3140 2981 0 02:05 pts/1 00:00:00 tail -F tail.logroot 3172 2919 0 02:19 pts/0 00:00:00 grep 2981[root@hadoop001 ~]# root 3140 2981 0 02:05 pts/1 00:00:00 tail -F tail.log每一个进程都有自己的id号，其中3140代表不唯一的id 2981是父id,是bash的id 3.杀死进程kill -9 pid杀死多个进程：kill -9 2873 3764pgrep -f tail 将tail过滤出来的进程打印出来[root@hadoop001 ~]# echo $(pgrep -f tail)将打印出来的进程由纵列变成横列[root@hadoop001 ~]# kill -9 $(pgrep -f tail)杀死关于tail命令的所有进程提醒：生产上假如非要执行kill杀进程，一定要确认清楚是否真的需要kill 4.端口号 123456789[root@hadoop001 ~]# ps -ef|grep sshroot 1475 1 0 Jun19 ? 00:00:00 /usr/sbin/sshdroot 2915 1475 0 01:09 ? 00:00:01 sshd: root@pts/0 root 2977 1475 0 01:28 ? 00:00:00 sshd: root@pts/1 root 3230 2919 0 02:42 pts/0 00:00:00 grep ssh[root@hadoop001 ~]# netstat -nlp|grep 1475tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1475/sshd tcp 0 0 :::22 :::* LISTEN 1475/sshd [root@hadoop001 ~]# netstat显示网络的状态ssh端口号不能为22，需要调整找到ssh的进程pid 1475，再通过pid找到该进程的端口号1475对外提供的服务是22，有时候一个进程有很多个端口号（对内），只有一个端口号（对外） 过滤进程的正常流程：进程名称–&gt;pid–&gt;port1.可以通过port检验端口号是否正确是否变更2.打开xx机器上面的httpd上的web界面，先要了解httpd的端口号是多少ps -ef|grep http 找网卡一种是ifconfig另一种是cat /etc/hosts 123[root@hadoop001 ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:0C:29:52:41:4B inet addr:192.168.0.130 Bcast:192.168.0.255 Mask:255.255.255.0 1234[root@hadoop001 ~]# service httpd startStarting httpd: httpd: apr_sockaddr_info_get() failed for hadoop001httpd: Could not reliably determine the server's fully qualified domain name, using 127.0.0.1 for ServerName [ OK ] 启动端口号 service httpd start 12345678910111213[root@hadoop001 ~]# ps -ef|grep httproot 1845 1816 3 20:43 pts/0 00:00:18 /usr/bin/python /usr/bin/yum install httpdroot 1900 1 0 20:51 ? 00:00:00 /usr/sbin/httpdapache 1903 1900 0 20:51 ? 00:00:00 /usr/sbin/httpdapache 1904 1900 0 20:51 ? 00:00:00 /usr/sbin/httpdapache 1905 1900 0 20:51 ? 00:00:00 /usr/sbin/httpdapache 1906 1900 0 20:51 ? 00:00:00 /usr/sbin/httpdapache 1907 1900 0 20:51 ? 00:00:00 /usr/sbin/httpdapache 1908 1900 0 20:51 ? 00:00:00 /usr/sbin/httpdapache 1909 1900 0 20:51 ? 00:00:00 /usr/sbin/httpdapache 1910 1900 0 20:51 ? 00:00:00 /usr/sbin/httpdroot 1918 1857 0 20:54 pts/1 00:00:00 grep http[root@hadoop001 ~]# 其中1900位为下面所有端口号的父进程，查看1900对外的端口号是80默认的访问是80端口号 123[root@hadoop001 ~]# netstat -nlp|grep 1900tcp 0 0 :::80 :::* LISTEN 1900/httpd [root@hadoop001 ~]# 对外提供服务的必须提供端口号，无端口号无法访问端口号对外服务的ip地址，假如为127.0.0.1或localhost,只能在这台的机器上访问这个服务一般这个地址为机器的IP或0.0.0.0或：：：，表示对外的任意IP可以服务 3.搜索：一种通过find命令搜索，一种通过history搜索find / -name ‘tail’find / -name ‘tail‘加’ ‘的是全称搜索加‘* ’号的是模糊匹配，例如：加号可以模糊匹配到abtail、abtailc、tailcfind /root -name ‘tail‘搜索root目录下面匹配的符合的tail文件或文件夹很少从根目录开始搜索，一般从特定的目录开始 4.安装rpm包安装：yum install xxx停服务：service xxx stop卸载：yum remove xxx安装mysql : yum search mysql安装mysql,mysql分为服务版和客户端，当不清晰服务端和客户端全称的时候，可以通过简写search,搜索mysql镜像源下面所匹配的库yum install mysql 会提示yes or no 如果不想输入的话，yum install -y mysql ,安装后显示安装的是libs,但真实需要是服务端mysql-server.x86_64,客户端mysql.x86_64Downloading Packages:(1/2): mysql-5.1.73-8.el6_8.x86_64.rpm | 895 kB 00:00(2/2): mysql-libs-5.1.73-8.el6_8.x86_64.rpm | 1.2 MB 00:00 1[root@hadoop001 ~]# yum install -y mysql-server.x86_64 mysql.x86_64 123[root@hadoop001 ~]# service httpd stopStopping httpd: [ OK ][root@hadoop001 ~]# yum remove httpd 进入mysql服务：service mysqld startmysql -uroot -p 不需要输入密码 1234567891011mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || test |+--------------------+3 rows in set (0.00 sec)mysql&gt; exit命令用于退出目前的shell。 执行exit可使shell以指定的状态值退出 12mysql&gt; exitBye rpm -qa搜索出这台机器中所有的软件包，过滤需要的mysql一种卸载：yum remove ,但是可以补全名字：yum remove mysql-server-5.1.73-8.el6_8.x86_64另一种卸载：rpm -e不校验依赖性卸载包的时候抛出依赖性，需要加参数–nodepsrpm -e –nodeps httpd-2.2.15-69.el6.centos.x86_64 5.which which指令会在环境变量$PATH设置的目录里查找符合条件的文件 which ls whereis ls java -version查看Java的版本 12345678[root@hadoop001 ~]# echo $PATH /usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin[root@hadoop001 ~]# which lsalias ls='ls --color=auto' /bin/ls[root@hadoop001 ~]# whereis ls ls: /bin/ls /usr/share/man/man1p/ls.1p.gz /usr/share/man/man1/ls.1.gz[root@hadoop001 ~]# 当ls执行的时候，去环境变量$PATH里面以：号为分隔，一层层查找 1234567891011121314151617[root@hadoop001 ~]# whereis ls ls: /bin/ls /usr/share/man/man1p/ls.1p.gz /usr/share/man/man1/ls.1.gz[root@hadoop001 ~]# cd /usr/lib64/qt-3.3/bin[root@hadoop001 bin]# lltotal 0[root@hadoop001 bin]# vi ls[root@hadoop001 bin]# chmod +x ls[root@hadoop001 bin]# [root@hadoop001 bin]# which lsalias ls='ls --color=auto' /usr/lib64/qt-3.3/bin/ls [root@hadoop001 bin]# ls ls [root@hadoop001 bin]# rm -f ls[root@hadoop001 bin]# which lsalias ls='ls --color=auto' /bin/ls 6.Win上传到Linux文件文件是压缩的文件或者是普通的文本文件，不会是文件夹 yum install lrzsz 文件从Win上传到Linux第一步 [root@hadoop001 bin]# yum install lrzsz第二步[root@hadoop001 bin]# rz弹窗里面选择window的文件进行传输sz filename 将Linux的文件传输给window设置属性-文件传输-选择下载路劲，文件传输时将不再次次询问 7.jdk的目录必须放在/usr/java如文件夹未创建mkdir -p /usr/java解压jdk的包，tar -xzvf jdk-8u45-linux-x64.gz -C /usr/java/解压后cd /usr/java/ 123456[root@hadoop001 ~]# tar -xzvf jdk-8u45-linux-x64.gz -C /usr/java/[root@hadoop001 ~]# cd /usr/java/[root@hadoop001 java]# lltotal 4drwxr-xr-x. 8 uucp 143 4096 Apr 11 2015 jdk1.8.0_45[root@hadoop001 java]# JDK解压的时候用户和用户组发生变更 必须修正修改用户和用户组的命令：chown -R root:root /usr/java/jdk1.8.0_45修正后用户和用户组为root 8.配置全局环境变量vi etc/profile查看JAVA HOME路径灵活使用cd、pwdvi /etc/profile进入vi模式末行输入JAVA HOME=/usr/java/jdk1.8.0_45/ 并配置PATH=${JAVA_HOME}/bin:$PATH 配置新的环境变量一定追加到前面，配置到后面可能查找的是之前旧的 =号前后不能有空格source /etc/profilewhich javajava -versionwhereis java 9.查看IPwindow：ipconfigLinux：ifconfig服务对外ip:portyou有可能抛错误：connection refused 连接拒绝 用telnet ip port timeout 超时 用ping ip/hostnametelnet ip port window位置在控制面板-程序和功能-启动或关闭window功能-勾选Telnetlinux 先安装 yum install -y telnet然后在telnet端口号和IP 123456[root@hadoop001 ~]# which telnet/usr/bin/telnet[root@hadoop001 ~]# telnet 192.168.0.130 80 Trying 192.168.0.130...Connected to 192.168.0.130.Escape character is '^]'. 10.以hostname去连接[root@hadoop001 ~]# ping hadoop001ping: unknown host hadoop001[root@hadoop001 ~]#原因是映射关系未配置导致也就是机器的名称对应的IP未进行配置Linux配置方式：vi /etc/hosts 输入IP地址和机器名称window配置方式: C:\\Windows\\System32\\drivers\\etc\\hosts注意 没有权限修改 直接复制一份桌面上，修改后 拖拽 覆盖即可","link":"/2020/03/02/Linux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A403/"},{"title":"Linux基础命令04","text":"1、vi的使用（1）vi常用的快捷键|快捷键 |作用 ||–|–|| dd|删除当前行 || dG| 删除光标所在行及以下的所有行 ||ndd|删除光标所在行及以下的n-1行|| gg| 跳转到第一行的首字母 || G | 跳转到最后一行的首字母 ||shift+$ |行尾|| 0 | 行首 || gg+dG | 真正清空0字节，如果文件内容很大，加载会很慢 ||set nu|显示行号||set nonu |取消显示行号 | （2）vi进入命令模式进入vi命令模式：例如：vi .bash_profile进入命令模式—按i键进入编辑模式—按esc退出编辑模式—按shift+：输入wq保存退出。（3）清空文件一种是通过vi命令进行清空gg+dG整个文件清空，但是文件内容假如很大，加载肯定很慢另一种是cat “ ” &gt; install.log cat是找文件，文件是双引号，提示文件不存在，，应该是打印看install.log里面有没有内容，vi进入内容为空，但是外面确展示有一个字节，不属于真正的0字节，里面存在一个占位符，如果用shell脚本判断的话并不是真正的0字节。 echo “” &gt; innstall.log 伪清空1个字节 echo ‘’ &gt; install.log 伪清空1个字节-rw-r–r–. 1 root root 1 Jun 24 00:49 innstall.logcat /dev/null &gt; install.log 真正的清空 0字节（4）搜索shift+： /xxx 按n向下定位查找，按N向上定位查找想要跳转到最后1行编辑G shift+$ shift+a/i 2、权限chown 改变用户 用户组chmod 改变读写执 12drwxr-xr-x. 2 root root 4096 Apr 12 00:17 Downloads-rw-r--r--. 1 root root 0 Jun 17 23:56 error.log 三个字母为一组。第一个字母：d文件夹、 -文件、 l连接、r 读4、w 写2、x 执行1（一般是shell脚本）、- 0 没有任何权限例如：rwxr-xr-x 第一组代表文件和文件夹所属的用户的权限:读写执第二组 r-x 5 代表文件和文件夹所属的 用户组的权限: 读执第三组 r-x 5 代表其他组的所有用户对这个文件或文件夹权限: 读执 123456789[root@hadoop001 photo]# touch test.log[root@hadoop001 photo]# lltotal 0-rw-r--r--. 1 root root 0 Jun 24 21:51 test.log[root@hadoop001 photo]# vi test.log[root@hadoop001 photo]# chmod 444 test.log[root@hadoop001 photo]# lltotal 4-r--r--r--. 1 root root 4 Jun 24 21:52 test.log 针对文件夹操作chown -R jepson:jepson ruozedatachmod -R 777 ruozedata针对文件夹的-R的参数就是chown chmod这俩个命令777代表所有人都有最大权限 读写执 4、一台服务器不可能只运行一个服务，例如：mysql对应的用户去运行mysql ,mysqladmin用户5、软链接：快捷键路径升级会用到软链接 12345678910[Wendy1@hadoop001 ~]$ mkdir Wendyv1.0[Wendy1@hadoop001 ~]$ lltotal 4drwxr-xr-x. 2 Wendy1 bigdata 4096 Jun 25 23:36 Wendyv1.0[Wendy1@hadoop001 ~]$ ln -s Wendyv1.0 rz[Wendy1@hadoop001 ~]$ lltotal 4lrwxrwxrwx. 1 Wendy1 bigdata 9 Jun 25 23:37 rz -&gt; Wendyv1.0drwxr-xr-x. 2 Wendy1 bigdata 4096 Jun 25 23:36 Wendyv1.0[Wendy1@hadoop001 ~]$ ln -s软连接，rz快捷文件夹路径，Wendyv1.0第一个路径代表了原始路径, rz -&gt; Wendyv1.0rz指向的原始路径软连接的使用场景：一是多版本，二是硬盘 查看磁盘空间：df -h内存：free -mtop: load average: 0.00, 0.00, 0.00 1min 5min 15min 生产上&lt;=10 代表系统不行 &gt;10 系统负载高 卡立即关机：shutdown -n now重启：reboot 6.解压缩zip -rzip -r test.zip test/* 12345678910111213141516171819202122232425262728[root@hadoop001 wendy2.0]# mkdir test[root@hadoop001 wendy2.0]# cd test/[root@hadoop001 test]# touch 1.log[root@hadoop001 test]# cd ../[root@hadoop001 wendy2.0]# lltotal 4drwxr-xr-x. 2 root root 4096 Jun 26 01:24 test[root@hadoop001 wendy2.0]# zip -r test.zip test/* adding: test/1.log (stored 0%)[root@hadoop001 wendy2.0]# lltotal 8drwxr-xr-x. 2 root root 4096 Jun 26 01:24 test-rw-r--r--. 1 root root 170 Jun 26 01:25 test.zip[root@hadoop001 wendy2.0]# rm -rf test[root@hadoop001 wendy2.0]# lltotal 4-rw-r--r--. 1 root root 170 Jun 26 01:25 test.zip[root@hadoop001 wendy2.0]# unzip test.zipArchive: test.zip extracting: test/1.log [root@hadoop001 wendy2.0]# lltotal 8drwxr-xr-x. 2 root root 4096 Jun 26 01:27 test-rw-r--r--. 1 root root 170 Jun 26 01:25 test.zip[root@hadoop001 wendy2.0]# ll testtotal 0-rw-r--r--. 1 root root 0 Jun 26 01:24 1.log[root@hadoop001 wendy2.0]# tar.gztar -xzvf xxx.tar.gztar -czvf xxx.tar.gz test/*查看命令帮助里面没有f和z，z代表的是它的后缀.gz ,x是解压，c是压缩 1234567891011[root@hadoop001 wendy2.0]# tar -czvf test.tar.gz test/*test/1.log[root@hadoop001 wendy2.0]# lltotal 12drwxr-xr-x. 2 root root 4096 Jun 26 01:27 test-rw-r--r--. 1 root root 115 Jun 26 01:34 test.tar.gz-rw-r--r--. 1 root root 170 Jun 26 01:25 test.zip[root@hadoop001 wendy2.0]# rm -rf test[root@hadoop001 wendy2.0]# tar -xzvf test.tar.gztest/1.log[root@hadoop001 wendy2.0]# 7.wget wget url下载连接地址8.调度crontab[root@hadoop001 tmp]# crontab -l /tmp/date.sh &gt;&gt; /tmp/date.log5个号分别代表：分 小时 日 月 周每隔两分钟的表示方式：/2 * * * *每隔10秒执行：12345678for((i=1;i&lt;=6;i++))do echo &quot;wwww.ruozedata.com&quot; date sleep 10sdoneexit 9.后台执行./date.sh &amp; 并不是真正后台执行 可能是会话nohup ./date.sh &amp;nohup ./date.sh &gt;&gt; /tmp/date.log 2&gt;&amp;1 &amp;","link":"/2020/03/02/Linux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A404/"},{"title":"Linux基础命令01","text":"1.路径pwd 查看当前文件路径，从根目录开始 linux系统的目录是从根目录开始的 / 代表的是根目录 2.用户窥探root 超级管理员 /root默认所有用户家目录除了root就都在home目录下面，除非指定用户的家目录为其他目录 3.查看执行ls /显示该目录下的文件夹或文件的名称执行ls -l /显示该目录下的文件夹或文件的明细信息 4.切换目录 cd /home cd /root cd ~ 切换到家目录 cd - 回到上一次目录 cd ../ 回退上一层目录 cd ../../ 回退两个目录 输入命令cd直接回车也可以快速切换root家目录 家目录的标识是~ 图片 5.看文件或文件夹大小 ls -l ==&gt; ll（ls -l显示该目录下的文件夹或文件的明细信息） ll -h 查看文件夹下面文件的大小 du -sh 查看文件夹或者文件大小 ll -rt （也可ll -r -t ,或ls -l -r -t ,或ls -lrt） 按时间排序，可便于找出最新的文件和文件夹 ll -a 查看隐藏文件夹或文件 隐藏是以“ .” 开始 例如：cd .ssh/ 6.创建 mkdir 创建一个新文件夹 mkdir -p demo/1/2 级联创建文件夹 （串行） mkdir 4 5 6 根目录下面创建4、5、6这三个并行的文件夹 （并行） 串行和并行的区别在 -p 创建文件 touch touch 创建0字节文件 vi xxx.log创建文件 图片图片 7.拷贝、移动cp:2份文件，原来的文件还在mv:1份文件，将原来的文件移动到新的位置mv xxx.log 8移动文件到新的文件位置mv photo1.log 7/photo123.log 文件移动的时候也可调整文件名称mv 8 7移动文件夹 8查看命令帮助 –help 9.查看文件内容 离线查看 cat 文件内容一下子全部显示 适用于内容少的文件内容查看 more 文件内容一页一页的往下翻 按空格键往下，无法回退，q键退出 适用于稍微多的内容 less 文件内容一行一行按箭头上下，q键退出 文件内容的ERROR: 较大的文件内容，发送给window电脑，通过editplus 进行全局搜索 文件超100M及以上 cat install.log | grep -C 10 “ERROR” -C指的是“ERROR”这个关键词上下十行 （常用） cat install.log | grep -a 10 “ERROR” -a指的“ERROR”这个关键词后十行 cat install.log | grep -b 10 “ERROR” -b指的“ERROR”这个关键词前十行 文件内容超多，筛选出的结果输出到日志内容里面 cat install.log | grep -C 10 “ERROR” &gt; error.log | 为管道符 &gt; 为重定向输出 覆盖 &gt;&gt; 追加实时查看-F=-f+retry 使用tail -f监控某个文件，在另一个窗口将该文件删除后，然后再新创建，tail -f的监制失效。tail -F则会再次进行监控tail -f Wendy1.logtail -F Wendy2.logecho “123” &gt;&gt; tail2.log echo打印，把这句话打印出来追加到tail2.log文件","link":"/2020/03/01/Linux%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A401/"},{"title":"MySQL语法02","text":"1、order by 排序排序order by 滞后，asc升序（升序一般是默认的）、desc降序order by后面可跟多个字段排序 1234567select num,name,age from ruozedata.studentinfo where name like 'r%' order by num asc ;#降序select num,name,age from ruozedata.studentinfo where name like 'r%' order by num desc ;#按部门编号排序且按月薪从大到小排序,select * from xxx order by depton , sal desc; 2、limit 限制多少行、 whereselect * from xxx，查询千万数据的表，有可能拖垮数据库。DBeaver，默认执行200条 1select * from xxx where deptno=10 limit 2; 注：如果生产环节直执行了select条件，未加where或linit,让DBA或者运维，执行下面命令： #查看进程，并kill进程#JDBC 确实需要拿1KW，普通写法肯定不行！ 用流式写法读取数据show processlists;kill xxx 3、group by 1.与聚合函数 sum count avg max min组合使用2.分组语法 select xxx,sum(yyy) from t group by xxx3.group by出现的字段 务必出现在 select 后面4.having 过滤 等价于 子表+where 1234567找薪水和＞9000的是哪个部门？ #过滤 havingselectdeptno,sum(sal) as saalfrom empgroup by deptnohaving saal &gt;9000; 4、join 多表 123# left join 以左表为主 a&lt;--b a数据最全 b是匹配 匹配多少算多少 on就是匹配条件#right join 以右表为主 a--&gt;b b数据最全 a是匹配 匹配多少算多少 on就是匹配条件#inner join 是将两张表的数据都存在 5、union#union all 结果不去重复 union结果去重复 注意：数量相同 类型相同 123select bid as id from testbunion select aid from testa; 6、topN 123456789101112131415161718192021#哪些部门的哪些职业的薪水和，最高1位的职业是什么？create view salas selectdeptno,job,sum(sal+ifnull(comm,0)) as salfrom emp group by deptno,job;select * from sal;select a.*from sal a where(select count(*) from sal b where a.deptno=b.deptnoand a.sal&lt;b.sal) =0order by a.deptno;","link":"/2020/03/03/MySQL%E8%AF%AD%E6%B3%9502/"},{"title":"MapReduce的工作原理图文详解","text":"开始聊mapreduce，mapreduce是hadoop的计算框架，我学hadoop是从hive开始入手，再到hdfs，当我学习hdfs时候，就感觉到hdfs和mapreduce关系的紧密。这个可能是我做技术研究的思路有关，我开始学习某一套技术总是想着这套技术到底能干什么，只有当我真正理解了这套技术解决了什么问题时候，我后续的学习就能逐步的加快，而学习hdfs时候我就发现，要理解hadoop框架的意义，hdfs和mapreduce是密不可分，所以当我写分布式文件系统时候，总是感觉自己的理解肤浅，今天我开始写mapreduce了，今天写文章时候比上周要进步多，不过到底能不能写好本文了，只有试试再说了。 Mapreduce初析 Mapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输入（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。 我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。 Mapreduce的基础实例 讲解mapreduce运行原理前，首先我们看看mapreduce里的hello world实例WordCount,这个实例在任何一个版本的hadoop安装程序里都会有，大家很容易找到，这里我还是贴出代码，便于我后面的讲解，代码如下： 点击(此处)折叠或打开 /** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * &quot;License&quot;); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package org.apache.hadoop.examples; import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; public class WordCount { public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length != 2) { System.err.println(&quot;Usage: wordcount &quot;); System.exit(2); } Job job = new Job(conf, &quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } 如何运行它，这里不做累述了，大伙可以百度下，网上这方面的资料很多。这里的实例代码是使用新的api，大家可能在很多书籍里看到讲解mapreduce的WordCount实例都是老版本的api，这里我不给出老版本的api，因为老版本的api不太建议使用了，大家做开发最好使用新版本的api，新版本api和旧版本api有区别在哪里： 新的api放在：org.apache.hadoop.mapreduce,旧版api放在：org.apache.hadoop.mapred 新版api使用虚类，而旧版的使用的是接口，虚类更加利于扩展，这个是一个经验，大家可以好好学习下hadoop的这个经验。 其他还有很多区别，都是说明新版本api的优势，因为我提倡使用新版api，这里就不讲这些，因为没必要再用旧版本，因此这种比较也没啥意义了。 下面我对代码做简单的讲解，大家看到要写一个mapreduce程序，我们的实现一个map函数和reduce函数。我们看看map的方法： public void map(Object key, Text value, Context context) throws IOException, InterruptedException {…} 这里有三个参数，前面两个Object key, Text value就是输入的key和value，第三个参数Context context这是可以记录输入的key和value，例如：context.write(word, one);此外context还会记录map运算的状态。 对于reduce函数的方法： public void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException {…} reduce函数的输入也是一个key/value的形式，不过它的value是一个迭代器的形式Iterable values，也就是说reduce的输入是一个key对应一组的值的value，reduce也有context和map的context作用一致。 至于计算的逻辑就是程序员自己去实现了。 下面就是main函数的调用了，这个我要详细讲述下，首先是： Configuration conf = new Configuration(); 运行mapreduce程序前都要初始化Configuration，该类主要是读取mapreduce系统配置信息，这些信息包括hdfs还有mapreduce，也就是安装hadoop时候的配置文件例如：core-site.xml、hdfs-site.xml和mapred-site.xml等等文件里的信息，有些童鞋不理解为啥要这么做，这个是没有深入思考mapreduce计算框架造成，我们程序员开发mapreduce时候只是在填空，在map函数和reduce函数里编写实际进行的业务逻辑，其它的工作都是交给mapreduce框架自己操作的，但是至少我们要告诉它怎么操作啊，比如hdfs在哪里啊，mapreduce的jobstracker在哪里啊，而这些信息就在conf包下的配置文件里。 接下来的代码是： String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length != 2) { System.err.println(&quot;Usage: wordcount &quot;); System.exit(2); } If的语句好理解，就是运行WordCount程序时候一定是两个参数，如果不是就会报错退出。至于第一句里的GenericOptionsParser类，它是用来解释常用hadoop命令，并根据需要为Configuration对象设置相应的值，其实平时开发里我们不太常用它，而是让类实现Tool接口，然后再main函数里使用ToolRunner运行程序，而ToolRunner内部会调用GenericOptionsParser。 接下来的代码是： Job job = new Job(conf, &quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); 第一行就是在构建一个job，在mapreduce框架里一个mapreduce任务也叫mapreduce作业也叫做一个mapreduce的job，而具体的map和reduce运算就是task了，这里我们构建一个job，构建时候有两个参数，一个是conf这个就不累述了，一个是这个job的名称。 第二行就是装载程序员编写好的计算程序，例如我们的程序类名就是WordCount了。这里我要做下纠正，虽然我们编写mapreduce程序只需要实现map函数和reduce函数，但是实际开发我们要实现三个类，第三个类是为了配置mapreduce如何运行map和reduce函数，准确的说就是构建一个mapreduce能执行的job了，例如WordCount类。 第三行和第五行就是装载map函数和reduce函数实现类了，这里多了个第四行，这个是装载Combiner类，这个我后面讲mapreduce运行机制时候会讲述，其实本例去掉第四行也没有关系，但是使用了第四行理论上运行效率会更好。 接下来的代码： job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); 这个是定义输出的key/value的类型，也就是最终存储在hdfs上结果文件的key/value的类型。 最后的代码是： FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); 第一行就是构建输入的数据文件，第二行是构建输出的数据文件，最后一行如果job运行成功了，我们的程序就会正常退出。FileInputFormat和FileOutputFormat是很有学问的，我会在下面的mapreduce运行机制里讲解到它们。 好了，mapreduce里的hello word程序讲解完毕，我这个讲解是从新办api进行，这套讲解在网络上还是比较少的，应该很具有代表性的。 Mapreduce运行机制 下面我要讲讲mapreduce的运行机制了，前不久我为公司出了一套hadoop面试题，里面就问道了mapreduce运行机制，出题时候我发现这个问题我自己似乎也将不太清楚，因此最近几天恶补了下，希望在本文里能说清楚这个问题。 下面我贴出几张图，这些图都是我在百度图片里找到的比较好的图片： 图片一： 图片二： 图片三： 图片四： 图片五： 图片六： 我现在学习技术很喜欢看图，每次有了新理解就会去看看图，每次都会有新的发现。 谈mapreduce运行机制，可以从很多不同的角度来描述，比如说从mapreduce运行流程来讲解，也可以从计算模型的逻辑流程来进行讲解，也许有些深入理解了mapreduce运行机制还会从更好的角度来描述，但是将mapreduce运行机制有些东西是避免不了的，就是一个个参入的实例对象，一个就是计算模型的逻辑定义阶段，我这里讲解不从什么流程出发，就从这些一个个牵涉的对象，不管是物理实体还是逻辑实体。 首先讲讲物理实体，参入mapreduce作业执行涉及4个独立的实体： 客户端（client）：编写mapreduce程序，配置作业，提交作业，这就是程序员完成的工作； JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业的执行； TaskTracker：保持与JobTracker的通信，在分配的数据片段上执行Map或Reduce任务，TaskTracker和JobTracker的不同有个很重要的方面，就是在执行任务时候TaskTracker可以有n多个，JobTracker则只会有一个（JobTracker只能有一个就和hdfs里namenode一样存在单点故障，我会在后面的mapreduce的相关问题里讲到这个问题的） Hdfs：保存作业的数据、配置信息等等，最后的结果也是保存在hdfs上面 那么mapreduce到底是如何运行的呢？ 首先是客户端要编写好mapreduce程序，配置好mapreduce的作业也就是job，接下来就是提交job了，提交job是提交到JobTracker上的，这个时候JobTracker就会构建这个job，具体就是分配一个新的job任务的ID值，接下来它会做检查操作，这个检查就是确定输出目录是否存在，如果存在那么job就不能正常运行下去，JobTracker会抛出错误给客户端，接下来还要检查输入目录是否存在，如果不存在同样抛出错误，如果存在JobTracker会根据输入计算输入分片（Input Split），如果分片计算不出来也会抛出错误，至于输入分片我后面会做讲解的，这些都做好了JobTracker就会配置Job需要的资源了。分配好资源后，JobTracker就会初始化作业，初始化主要做的是将Job放入一个内部的队列，让配置好的作业调度器能调度到这个作业，作业调度器会初始化这个job，初始化就是创建一个正在运行的job对象（封装任务和记录信息），以便JobTracker跟踪job的状态和进程。初始化完毕后，作业调度器会获取输入分片信息（input split），每个分片创建一个map任务。接下来就是任务分配了，这个时候tasktracker会运行一个简单的循环机制定期发送心跳给jobtracker，心跳间隔是5秒，程序员可以配置这个时间，心跳就是jobtracker和tasktracker沟通的桥梁，通过心跳，jobtracker可以监控tasktracker是否存活，也可以获取tasktracker处理的状态和问题，同时tasktracker也可以通过心跳里的返回值获取jobtracker给它的操作指令。任务分配好后就是执行任务了。在任务执行时候jobtracker可以通过心跳机制监控tasktracker的状态和进度，同时也能计算出整个job的状态和进度，而tasktracker也可以本地监控自己的状态和进度。当jobtracker获得了最后一个完成指定任务的tasktracker操作成功的通知时候，jobtracker会把整个job状态置为成功，然后当客户端查询job运行状态时候（注意：这个是异步操作），客户端会查到job完成的通知的。如果job中途失败，mapreduce也会有相应机制处理，一般而言如果不是程序员程序本身有bug，mapreduce错误处理机制都能保证提交的job能正常完成。 下面我从逻辑实体的角度讲解mapreduce运行机制，这些按照时间顺序包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。 输入分片（input split）：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切，假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），换句话说我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。 map阶段：就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行； combiner阶段：combiner阶段是程序员可以选择的，combiner其实也是一种reduce操作，因此我们看见WordCount类里是用reduce进行加载的。Combiner是一个本地化的reduce操作，它是map运算的后续操作，主要是在map计算出中间文件前做一个简单的合并重复key值的操作，例如我们对文件里的单词频率做统计，map计算时候如果碰到一个hadoop的单词就会记录为1，但是这篇文章里hadoop可能会出现n多次，那么map输出文件冗余就会很多，因此在reduce计算前对相同的key做一个合并操作，那么文件会变小，这样就提高了宽带的传输效率，毕竟hadoop计算力宽带资源往往是计算的瓶颈也是最为宝贵的资源，但是combiner操作是有风险的，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。 shuffle阶段：将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。这里我不讲怎么优化shuffle阶段，讲讲shuffle阶段的原理，因为大部分的书籍里都没讲清楚shuffle阶段。Shuffle一开始就是map阶段做输出操作，一般mapreduce计算的都是海量数据，map输出时候不可能把所有文件都放到内存操作，因此map写入磁盘的过程十分的复杂，更何况map输出时候要对结果进行排序，内存开销是很大的，map在做输出时候会在内存里开启一个环形内存缓冲区，这个缓冲区专门用来输出的，默认大小是100mb，并且在配置文件里为这个缓冲区设定了一个阀值，默认是0.80（这个大小和阀值都是可以在配置文件里进行配置的），同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阀值的80%时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill，另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作是互不干扰的，如果缓存区被撑满了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作，前面我讲到写入磁盘前会有个排序操作，这个是在写入磁盘操作时候进行，不是在写入内存时候进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。每次spill操作也就是写入磁盘操作时候就会写一个溢出文件，也就是说在做map输出有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。这个过程里还会有一个Partitioner操作，对于这个操作很多人都很迷糊，其实Partitioner操作和map阶段的输入分片（Input split）很像，一个Partitioner对应一个reduce作业，如果我们mapreduce操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片，这个程序员可以编程控制，主要是根据实际key和value的值，根据实际业务类型或者为了更好的reduce负载均衡要求进行，这是提高reduce效率的一个关键所在。到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，程序员也可以在配置文件更改复制线程的个数，这个复制过程和map写入磁盘过程类似，也有阀值和内存大小，阀值一样可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。 reduce阶段：和map函数一样也是程序员编写的，最终结果是存储在hdfs上的。 Mapreduce的相关问题 这里我要谈谈我学习mapreduce思考的一些问题，都是我自己想出解释的问题，但是某些问题到底对不对，就要广大童鞋帮我确认了。 jobtracker的单点故障：jobtracker和hdfs的namenode一样也存在单点故障，单点故障一直是hadoop被人诟病的大问题，为什么hadoop的做的文件系统和mapreduce计算框架都是高容错的，但是最重要的管理节点的故障机制却如此不好，我认为主要是namenode和jobtracker在实际运行中都是在内存操作，而做到内存的容错就比较复杂了，只有当内存数据被持久化后容错才好做，namenode和jobtracker都可以备份自己持久化的文件，但是这个持久化都会有延迟，因此真的出故障，任然不能整体恢复，另外hadoop框架里包含zookeeper框架，zookeeper可以结合jobtracker，用几台机器同时部署jobtracker，保证一台出故障，有一台马上能补充上，不过这种方式也没法恢复正在跑的mapreduce任务。 做mapreduce计算时候，输出一般是一个文件夹，而且该文件夹是不能存在，我在出面试题时候提到了这个问题，而且这个检查做的很早，当我们提交job时候就会进行，mapreduce之所以这么设计是保证数据可靠性，如果输出目录存在reduce就搞不清楚你到底是要追加还是覆盖，不管是追加和覆盖操作都会有可能导致最终结果出问题，mapreduce是做海量数据计算，一个生产计算的成本很高，例如一个job完全执行完可能要几个小时，因此一切影响错误的情况mapreduce是零容忍的。 Mapreduce还有一个InputFormat和OutputFormat，我们在编写map函数时候发现map方法的参数是之间操作行数据，没有牵涉到InputFormat，这些事情在我们new Path时候mapreduce计算框架帮我们做好了，而OutputFormat也是reduce帮我们做好了，我们使用什么样的输入文件，就要调用什么样的InputFormat，InputFormat是和我们输入的文件类型相关的，mapreduce里常用的InputFormat有FileInputFormat普通文本文件，SequenceFileInputFormat是指hadoop的序列化文件，另外还有KeyValueTextInputFormat。OutputFormat就是我们想最终存储到hdfs系统上的文件格式了，这个根据你需要定义了，hadoop有支持很多文件格式，这里不一一列举，想知道百度下就看到了。 好了，文章写完了，呵呵，这篇我自己感觉写的不错，是目前hadoop系列文章里写的最好的，我后面会再接再厉的。加油！！！ 转: http://www.cnblogs.com/sharpxiajun/p/3151395.html","link":"/2020/03/03/MapReduce%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3/"},{"title":"MySQL语法01","text":"1、创建数据库 12#创建数据库 ruozedatacreate database ruozedata; 2、创建用户并刷新用户权限 12grant all privileges on ruozedata.* to ruoze@'%' identified by '123456';flush privileges; 3、基本权限查看 123456#查看数据库show create database ruozedata;#查看表结构show create table stuinfo;#查看用户权限show grants for ruoze@'%'; 4、MySQL字段类型 数字类型：int 整数，long 长整数、float 单精度 、double 双精度 、decimal 小数值字符串’abc’：char 定长字符串 0-255字节、varchar 变长字符串 0-65535字节、text日期和时间：date YYYY-MM-DD 2019-09-10time HH:MM:SS 10:10:10datetime 2019-09-10 10:10:10timestamp 2019-09-10 10:10:10 5、增删改查insert、update、delete、select insert into ruozedata(id,name,age) values(1,’rz’,18)update ruozedata set age=22 where id=1;delete from ruozedata where id=1;select * from ruozedata; 1234567891011#举例增删改查的实际应用create table ruozedata(id int ,name varchar(200),age int)insert into ruozedata(id,name,age) values(1,'rz',18)update ruozedata set age=22 where id=1;delete from ruozedata where id=1;select * from ruozedata; 6、语法DDL: 数据定义语言 create drop alterDML: 数据操作语言 insert update delete selectDCL: 数据控制语言 grant 1234567891011121314#删除表drop table ruozedata;#创建表create table 表名（）...#执行结果select * from ruozedata;#约束 default#1/主键 primary key 简写pk 一张表就只能一个主键==非空约束+唯一#2/主键是唯一约束的变态升级#3/主键或唯一约束可以多个字段设置，根据数据的特性ALTER TABLE ruozedata.studentinfo ADD CONSTRAINT studentinfo_un UNIQUE KEY (num,name，...) ;数据不能存在多条重复#多列update ruozedata.studentinfo set name='huhu2',age=19 where id=3; 12345678910111213141516171819# id一定要自增长 非业务字段。#创建人/创建时间/修改人/修改时间，任何的数据设计时都需要进行时间、人的记录。#字段命名统一风格，常用的就是英文单词加上下划线组合，字母通常是3/4个，如果已建表看表的规则。#不要将汉语拼音全拼或者缩写作为字段#不要将中文作为字段名称--------------------------------------示例：create table ruozedata(id int auto_increment primary key,stu_number int,stu_name varchar(200),stu_age int ,createtime timestamp default current_timestamp, #createtime创建时间 timestamp字段类型 default current_timestamp默认值当前时间cretae_user varchar(100),update_time timestamp default current_timestamp on update current_timestamp,#on update current_timestamp当表的数据发生变化，字段会自己做时间更新update_user varchar(100)) 关于where条件生产上 update 切记是否要加where条件生产上 delete 切记是否要加where条件update ruozedata.studentinfo set age=28 where num=1;update ruozedata.studentinfo set age=19; update不加where的情况1 1 jepson 19 2019-06-26 22:14:19 2019-06-26 22:24:172 2 ruoze 19 2019-06-26 22:20:10 2019-06-26 22:24:173 3 huhu 19 2019-06-26 22:20:10 r 2019-06-26 22:24:17 （1）插入数据 12345insert into 表名(字段名1，字段名2，……)values(值1，值2，……)示例：insert into ruozedata.stuinfo(num,name,age) values(1,'ruoze',12,……); （2）更改数据 123update 表名 set 字段名=新值 示例：update stuinfo set age=15 where id=1; （3）查询数据 1234# &gt; &lt; =select * from ruozedata.studentinfo where id&gt;=1; select * from ruozedata.studentinfo where name='ruoze1'; select * from ruozedata.studentinfo where name!='ruoze1'; 查询年龄26岁或名称ruoze1的数据，or表示或的意思查询年龄26岁且名称ruoze1的数据，and表示或的意思查询ruoze1和jepson之间年龄为26的select * from ruozedata.studentinfo where age=26 or name=’ruoze1’;select * from ruozedata.studentinfo where age=26 and name=’ruoze1’;select * from ruozedata.studentinfo where age=26 and (name=’ruoze1’ or name=’jepson’);模糊查询查询含有r字母的select * from ruozedata.studentinfowhere name like ‘%r%’;查询首字母为ruo的select * from ruozedata.studentinfowhere name like ‘ruo%’;查询最后字母select * from ruozedata.studentinfowhere name like ‘%1’;查询第三个字母为o,前两个字母用占位符__select * from ruozedata.studentinfowhere name like ‘__o%’; （4）删除数据 1delete from ruozedata where id=1;","link":"/2020/03/03/MySQL%E8%AF%AD%E6%B3%9501/"},{"title":"MySQL部署","text":"MySQL部署流程：通过rpm包或tar包部署企业里一般是通过tar包部署，可进行定制化 一、rpm包部署：一般机器上面都有mysql - libs包，不可无故卸载libs包，可能会导致其他依赖性服务不正常rpm -e mysql-libs-5.1.73-8.el6_8.x86_64 是可卸载rpm libs包，卸载的时候会告诉一个依赖，则使用rpm -e –nodeps进行卸载 安装流程： 1、yum search mysql 搜索MySQL文件 2、yum install -y mysql -server.x86_64安装server包 3、yum install -y mysql -server .x86_64 mysql .x86_64 同时安装client包 4、yum install -y mysql-libs-5.1.73-8.el6_8.x86_64安装libs 5、service mysqld start 进行启动校验 6、mysql 如果出现提示，不能连接本地的MySQL服务通过socket ‘/tmp/mysql .sock’(2)先查看后台进程 ps -ef|grep mysql ，会发现后台进行的进程是在–socket=/var/lib/mysql/mysql .sock下面，在命令后面加上连接指向mysql –socket=/var/lib/mysql/mysql .sock ，就可以登录到mysql 12345678910111213141516[root@hadoop001 ~]# service mysqld startStarting mysqld: [ OK ][root@hadoop001 ~]# mysqlWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 4Server version: 5.1.73 Source distributionCopyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; 二：tar包部署：1、rz上传tar包，将上传的tar包移动到 /usr/local 文件下面2、ps -ef|grep mysqld 检查是否存在残留进程，rpm -qa |grep -i mysql 搜索rpm包3、tar xzvf 解压tar包4、ln -s mysql-5.6.23-linux-glibc2.5-x86_64 mysql 设置软连接 1234[root@hadoop001 local]# tar xzvf mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz#解压[root@hadoop001 local]# ln -s /usr/local/mysql-5.6.23-linux-glibc2.5-x86_64 /usr/local/mysql#软链接创建 5、创建一个组合user，不同的组件服务是由它不同的专属用户一起来运行的，例如：mysql的用户用mysqladmin来运行，创建一个用户组groupadd -g 101 dba ，101指dba的group的id创建一个家目录在/usr/local/mysql主组为dba的用户mysqladmin 1234[root@hadoop001 local]# groupadd -g 101 dba[root@hadoop001 local]#useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladminuid=514(mysqladmin) gid=101(dba) groups=101(dba),0(root) # -u指的是514,514是用户的id，-g dba指的是主组是dba ， -G root指的是G属于root组 -d指的是家目录 /usr/local/mysql mysqladmin，如果没有-d默认是home目录下mysqladmin 6、创建配置文件 Create /etc/my.cnf(640)mysql的配置文件默认都是从/etc/my.cnf这个配置文件去读，一般这个文件都是会有内容存在，可以先将文件进行备份，mv /etc/my.cnf /etc/my.cnf20190629vi my.cnf先清空内容d+G，然后编辑模式将下面的代码内容复制粘贴生产上只需要调整innodb_buffer_pool_size = 2048M这个参数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798[client]port = 3306socket = /usr/local/mysql/data/mysql.sock [mysqld]port = 3306socket = /usr/local/mysql/data/mysql.sockskip-external-lockingkey_buffer_size = 256Msort_buffer_size = 2Mread_buffer_size = 2Mread_rnd_buffer_size = 4Mquery_cache_size= 32Mmax_allowed_packet = 16Mmyisam_sort_buffer_size=128Mtmp_table_size=32Mtable_open_cache = 512thread_cache_size = 8wait_timeout = 86400interactive_timeout = 86400max_connections = 600# Try number of CPU's*2 for thread_concurrencythread_concurrency = 32#isolation level and default engine default-storage-engine = INNODBtransaction-isolation = READ-COMMITTEDserver-id = 1basedir = /usr/local/mysqldatadir = /usr/local/mysql/datapid-file = /usr/local/mysql/data/hostname.pid#open performance schemalog-warningssysdate-is-nowbinlog_format = MIXEDlog_bin_trust_function_creators=1log-error = /usr/local/mysql/data/hostname.errlog-bin=/usr/local/mysql/arch/mysql-bin#other logs#general_log =1#general_log_file = /usr/local/mysql/data/general_log.err#slow_query_log=1#slow_query_log_file=/usr/local/mysql/data/slow_log.err#for replication slave#log-slave-updates #sync_binlog = 1#for innodb options innodb_data_home_dir = /usr/local/mysql/data/innodb_data_file_path = ibdata1:500M:autoextendinnodb_log_group_home_dir = /usr/local/mysql/archinnodb_log_files_in_group = 2innodb_log_file_size = 200Minnodb_buffer_pool_size = 2048Minnodb_additional_mem_pool_size = 50Minnodb_log_buffer_size = 16Minnodb_lock_wait_timeout = 100#innodb_thread_concurrency = 0innodb_flush_log_at_trx_commit = 1innodb_locks_unsafe_for_binlog=1#innodb io features: add for mysql5.5.8performance_schemainnodb_read_io_threads=4innodb-write-io-threads=4innodb-io-capacity=200#purge threads change default(0) to 1 for purgeinnodb_purge_threads=1innodb_use_native_aio=on#case-sensitive file names and separate tablespaceinnodb_file_per_table = 1lower_case_table_names=1[mysqldump]quickmax_allowed_packet = 16M[mysql]no-auto-rehash[mysqlhotcopy]interactive-timeout[myisamchk]key_buffer_size = 256Msort_buffer_size = 256Mread_buffer = 2Mwrite_buffer = 2M 7、修改权限，修改配置文件所属的用户和用户组关于文件夹的修改 修改完成后 一定要cd进去 ll再看一下 123456[root@hadoop001 local]# chown mysqladmin:dba /etc/my.cnf [root@hadoop001 local]# chmod 755 /etc/my.cnf [root@hadoop001 local]# chown -R mysqladmin:dba /usr/local/mysql[root@hadoop001 local]# chown -R mysqladmin:dba /usr/local/mysql/*[root@hadoop001 local]# chmod -R 755 /usr/local/mysql [root@hadoop001 local]# chmod -R 755 /usr/local/mysql/* 8、创建binlog日志存储的文件夹检查是否安装libaio包 123[mysqladmin@hadoop001 ~]$ mkdir arch [mysqladmin@hadoop001 ~]$ yum -y install autoconf[mysqladmin@hadoop001 ~]$ yum -y install libaio 9、真正安装命令scripts/mysql_install_db –user=mysqladmin –basedir=/usr/local/mysql –datadir=/usr/local/mysql/data 10、配置MySQL服务并启动 1234567891011121314151617181920[root@hadoop001 local]# cd /usr/local/mysql#将服务文件拷贝到init.d下，并重命名为mysql[root@hadoop001 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql #赋予可执行权限[root@hadoop001 mysql]# chmod +x /etc/rc.d/init.d/mysql#删除服务[root@hadoop001 mysql]# chkconfig --del mysql#添加服务[root@hadoop001 mysql]# chkconfig --add mysql[root@hadoop001 mysql]# chkconfig --level 345 mysql on[root@hadoop001 mysql]# vi /etc/rc.local#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don't# want to do the full Sys V style init stuff.touch /var/lock/subsys/localsu - mysqladmin -c &quot;/etc/init.d/mysql start&quot; &lt;--添加这一行 11、启动MySQL服务并验证 12345678910111213141516[root@hadoop001 mysql]# su - mysqladmin[mysqladmin@hadoop001 ~]$ pwd/usr/local/mysql[mysqladmin@hadoop001 ~]$ rm -rf my.cnf[mysqladmin@hadoop001 ~]$ ps -ef|grep mysqld514 6247 6219 0 17:30 pts/1 00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe514 6902 6247 2 17:30 pts/1 00:00:01 /usr/local/mysql/bin/mysqld --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --plugin-dir=/usr/local/mysql/lib/plugin --log-error=/usr/local/mysql/data/hostname.err --pid-file=/usr/local/mysql/data/hostname.pid --socket=/usr/local/mysql/data/mysql.sock --port=3306514 6927 6219 0 17:31 pts/1 00:00:00 grep mysqld[mysqladmin@hadoop001 ~]$ netstat -tulnp | grep 6902tcp 0 0 :::3306 :::* LISTEN 11541/mysqld [root@hadoop001 local]# service mysql statusMySQL running (21507) [ OK ] 12、登录MySQL 12345678910111213141516171819202122232425262728293031323334353637383940[mysqladmin@hadoop001 ~]$ mysqlWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 1Server version: 5.6.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || test |+--------------------+4 rows in set (0.00 sec)mysql&gt; use mysql；Database changedmysql&gt; update user set password=password('password') where user='root';Query OK, 4 rows affected (0.00 sec)mysql&gt; delete from user where user='';Query OK, 2 rows affected (0.00 sec)mysql&gt; select host,user,password from user;+----------------+------+-------------------------------------------+| host | user | password |+----------------+------+-------------------------------------------+| localhost | root | *6340BE3C15D246B0D74BAF3F135915ED19E0069F || hadoop001 | root | *6340BE3C15D246B0D74BAF3F135915ED19E0069F || 127.0.0.1 | root | *6340BE3C15D246B0D74BAF3F135915ED19E0069F || ::1 | root | *6340BE3C15D246B0D74BAF3F135915ED19E0069F |+----------------+------+-------------------------------------------+4 rows in set (0.00 sec)mysql&gt; flush privileges; 13、配置全局环境变量 123456[root@hadoop001 ~]# vi /etc/profile[root@hadoop001 ~]# source /etc/profile#环境变量里新增内容JAVA_HOME=/usr/java/jdk1.8.0_45MYSQL_HOME=/usr/local/mysqlPATH=${JAVA_HOME}/bin:$PATH","link":"/2020/03/03/MySQL%E9%83%A8%E7%BD%B2/"},{"title":"Spark RDD算子","text":"一、Transformations与Actions主要算子（1）Transformations|算子 |描述 ||–|–|| map | 处理每一条数据 || mapPartitions | 对每个分区进行处理 || mapPartitionsWithIndex | 每个元素跟所在分区形成一个元组 || mapValues| 是针对RDD[K,V]的V做处理 || flatmap | flatmap = map + flatten || glom |每一个分区的数据放在一个数组 || sample | || filter | 留下满足条件的 ||groupKeyKey | RDD[K,V]按传入函数的返回值进行分组，key值相同为一组 ||distinct |去重 || groupBy | 自定义分组 分组条件就是自定义传进去的 || sortBy | 排序，按正序排| （2）Actions|算子 |描述 ||–|–|| collect | 数组的形式返回数据集的所有元素 || foreach|打印每个元素 ||count |统计RDD条数 || reduce | 聚合rdd中的元素，rdd.reduce(+)就是每个元素相加 || first | 取元素里面的第一个元素，底层调用的是take方法 || take(n) | 前n个元素组成的数组 || top(n) | 返回最大的前n个元素，底层调用的是takeOrdered || takeOrdered|排序后的前n个元素组成的数组 ||countByKey |每种key的个数||countByKey |每种key的个数|","link":"/2020/03/03/Spark%20RDD%E7%AE%97%E5%AD%90/"},{"title":"YARN HA架构","text":"一、伪分布式Yarn只有rm、nm进程伪分布式部署或者是集群部署不做HA的话，rm是单点的，如果做HA，rm会有2个，比如rm1、rm2 rm1 activerm2 standby 这个时候机器上的进程是 ruozedata001: ZK NN zkfc jn DN RM NMruozedata002: ZK NN zkfc jn DN RM NMruozedata003: ZK jn DN NM 二、YARN HA架构图Yarn HARM:a.启动时会通过向ZK的/hadoop-ha目录写一个lock文件，写成功则为active，否则standby。standby RM会一直监控lock文件的是否存在，如果不存在就会尝试去创建，争取为active rm。b.会接收客户端的任务请求，接收和监控nm的资源的汇报，负责资源的分配与调度，启动和监控 ApplicationMaster（AM） NM:节点上的资源的管理，启动container 容器 运行task的计算，上报资源，container情况汇报给RM和任务的处理情况汇报给 ApplicationMaster（AM） ApplicationMaster（AM）driver : nm机器上的container单个application(job)的task的管理和调度，并向rm进行资源的申请，向nm发出 launch container指令，接收NM的task的处理状态信息。 RMstatestore:a.RM的作业信息存储在ZK的/rmstore下，active RM向这个目录写app信息b.当active rm挂了，另外一个standby rm成功转换为active rm后，会从/rmstore目录读取相应的作业信息，重新构建作业的内存信息。然后启动内部服务，开始接收NM的心跳，构建集群资源的信息，并接收客户端的提交作业的请求等。 ZKFC:自动故障转移 只作为RM进程的一个线程 而非独立的守护进程来启动","link":"/2020/03/03/YARN%20HA%E6%9E%B6%E6%9E%84/"},{"title":"Spark的分布式数据集RDD","text":"一.RDD介绍(1)RDD介绍RDD全称resilient distributed dataset（弹性分布式数据集）。他是一个弹性分布式数据集，是spark里面抽象的概念。代表的是一个不可变的，集合里面的元素可以分区的支持并行化的操作。(2)源码描述RDD的五大特性 A list of partitions A function for computing each split A list of dependencies on other RDDs Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) Optionally, a list of preferred locations to compute each split on (e.g. block locations foran HDFS file)(3)五大特性对应的方法 1234567891011121314151）A list of partitions 一系列的partition protected def getPartitions: Array[Partition]2）A function for computing each split 针对RDD做操作其实就是针对RDD底层的partition进行操作 rdd.map(_*2) def compute(split: Partition, context: TaskContext): Iterator[T]3）A list of dependencies on other RDDs rdd之间的依赖 protected def getDependencies: Seq[Dependency[_]] = deps4）Optionally, a Partitioner for key-value RDDs partitioner rdd的分片函数 @transient val partitioner: Option[Partitioner] = None5）Optionally, a list of preferred locations to compute each split on locations protected def getPreferredLocations(split: Partition): Seq[String] = Nil 二、开发spark应用程序 1）SparkConf需要设置 appName master 2）SparkContext(sparkConf) 3）spark-shell 启动时，使用master设置 –master local[2] 底层自动为我们创建了SparkContext 三、RDD创建方式 1）parallelize 适用于测试 2）External Datasets 生产 3）通过已有RDD转换过来的 生产 12345678910111）parallelizescala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; rdd.collectres0: Array[Int] = Array(1, 2, 3, 4, 5)2）External Datasetsscala&gt; val rdd = sc.textFile(&quot;file:///home/hadoop/data/wc.data&quot;)3）通过已有RDD转换过来的scala&gt; val rdd1 = rdd.map(_*2) 四、RDD操作transformation 转换action 动作 触发 SparkContext 提交 作业transformation将一个RDD转换成另一个RDD并不会立即执行，只有遇到action才会提交作业开始执行计算 12345678910111213141516举例：transformation- map：对数据集的每一个元素进行操作- mapPartitions: 对每个分区进行处理- mapValues 是针对RDD[K,V]的V做处理- FlatMap：先对数据集进行扁平化处理，然后再Map- Filter：对数据进行过滤，为true则通过- destinct：去重操作action- reduce：对数据进行聚集- reduceBykey：对key值相同的进行操作- collect：没有效果的action，但是很有用- saveAstextFile：数据存入文件系统- foreach：对每个元素进行func的操作 rdd.mappartitions(_*2)是错误的map是对每一条数据做处理的，mappartitions对分区的数据做处理的，分区内是有很多个int元素，是一个迭代的int假如有100个元素 10个分区 作用于map和mapPartitions==&gt; 知识点：要把RDD的数据写入MySQL Connection","link":"/2020/03/03/Spark%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86RDD/"},{"title":"Spark排序方式之OrderingOn","text":"一、Spark的几种排序方式（1）直接根据字段进行排序 弊端：如果字段太多不便于使用（2）封装一个类，实现它的Ordered 弊端：用自定义实现序列化方式比较麻烦（3）用case class的方式排序 推荐：不用new、默认就实现了序列化（4）最简单的排序方式Ordering on 二、Spark最简单的一种排序方式Ordering on 12345678910111213141516171819202122232425262728293031323334package com.ruozedata.bigdata.spark.core02import com.ruozedata.bigdata.spark.utils.ContextUtilsimport com.ruozedata.bigdata.spark.utils.ImplicitAspect._object SortApp { def main(args: Array[String]): Unit = { val sc = ContextUtils.getContext(this.getClass.getSimpleName)&lt;!--more--&gt; val products = sc.parallelize(List(&quot;啤酒 20 10&quot;,&quot;红酒 20 100&quot;,&quot;白酒 5 200&quot;)) val product = products.map(x =&gt; { val splits = x.split(&quot; &quot;) val name = splits(0) val price = splits(1).toDouble val amount = splits(2).toInt (name, price, amount) }) /** * Ordering on * * -x._2, -x._3 排序规则 * (Double,Int) 定义的是规则的返回值的类型 * (String,Double,Int) 数据的类型 */ implicit val ord = Ordering[(Double,Int)].on[(String,Double,Int)](x=&gt;(-x._2, -x._3)) product.sortBy(x=&gt;x).printInfo() sc.stop() }} 12345输出的结果(红酒,20.0,100)(啤酒,20.0,10)(白酒,5.0,200)~~~~~~~~~~~~~~~~~~~","link":"/2020/03/03/Spark%E6%8E%92%E5%BA%8F%E6%96%B9%E5%BC%8F%E4%B9%8BOrderingOn/"},{"title":"YARN架构设计","text":"yarn的工作流程 1、客户端向yarn提交job，jon里包含application master等程序，然后yarn会启动ApplicationsManager，让它来管理job 2、RM为该job分配第一个container,与对应的NM通信，，要求它在这个container启动作业的application master 3、ApplicationMaster会向ApplicationsManager注册，这样用户可以在页面上实时查看任务状态，一直到最后 4、application master采用轮询的方式通过RPC协议向resource scheduler申请和领取资源 5、在ApplicationMaster领取到资源后，会与对应的NM通信要求开启Task 6、NM为任务设置好运行环境后，将任务的启动命令写到一个脚本中，并通过该脚本启动并运行任务 7、各个Task通过RPC向ApplicationMaster汇报，以让application master随时掌握各个任务的运行状态，从而在任务失败时，重启启动任务 8、作业结束后，ApplicationMaster向ApplicationsManager注销并关闭自己","link":"/2020/03/03/YARN%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"},{"title":"YARN的Memory和CPU调优配置详解","text":"Hadoop YARN同时支持内存和CPU两种资源的调度，本文介绍如何配置YARN对内存和CPU的使用。 YARN作为一个资源调度器，应该考虑到集群里面每一台机子的计算资源，然后根据application申请的资源进行分配Container。Container是YARN里面资源分配的基本单位，具有一定的内存以及CPU资源。 在YARN集群中，平衡内存、CPU、磁盘的资源的很重要的，根据经验，每两个container使用一块磁盘以及一个CPU核的时候可以使集群的资源得到一个比较好的利用。内存配置 关于内存相关的配置可以参考hortonwork公司的文档Determine HDP Memory Configuration Settings来配置你的集群。 YARN以及MAPREDUCE所有可用的内存资源应该要除去系统运行需要的以及其他的hadoop的一些程序，总共保留的内存=系统内存+HBASE内存。可以参考下面的表格确定应该保留的内存：每台机子内存 系统需要的内存 HBase需要的内存4GB 1GB 1GB8GB 2GB 1GB16GB 2GB 2GB24GB 4GB 4GB48GB 6GB 8GB64GB 8GB 8GB72GB 8GB 8GB96GB 12GB 16GB128GB 24GB 24GB255GB 32GB 32GB512GB 64GB 64GB 计算每台机子最多可以拥有多少个container，可以使用下面的公式: containers = min (2CORES, 1.8DISKS, (Total available RAM) / MIN_CONTAINER_SIZE) 说明： CORES为机器CPU核数 DISKS为机器上挂载的磁盘个数 Total available RAM为机器总内存 MIN_CONTAINER_SIZE是指container最小的容量大小，这需要根据具体情况去设置，可以参考下面的表格：每台机子可用的RAM container最小值小于4GB 256MB4GB到8GB之间 512MB8GB到24GB之间 1024MB大于24GB 2048MB 每个container的平均使用内存大小计算方式为： RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers)) 通过上面的计算，YARN以及MAPREDUCE可以这样配置：配置文件 配置设置 默认值 计算值yarn-site.xml yarn.nodemanager.resource.memory-mb 8192 MB = containers * RAM-per-containeryarn-site.xml yarn.scheduler.minimum-allocation-mb 1024MB = RAM-per-containeryarn-site.xml yarn.scheduler.maximum-allocation-mb 8192 MB = containers * RAM-per-containeryarn-site.xml (check) yarn.app.mapreduce.am.resource.mb 1536 MB = 2 * RAM-per-containeryarn-site.xml (check) yarn.app.mapreduce.am.command-opts -Xmx1024m = 0.8 * 2 * RAM-per-containermapred-site.xml mapreduce.map.memory.mb 1024 MB = RAM-per-containermapred-site.xml mapreduce.reduce.memory.mb 1024 MB = 2 * RAM-per-containermapred-site.xml mapreduce.map.java.opts = 0.8 * RAM-per-containermapred-site.xml mapreduce.reduce.java.opts = 0.8 * 2 * RAM-per-container 举个例子：对于128G内存、32核CPU的机器，挂载了7个磁盘，根据上面的说明，系统保留内存为24G，不适应HBase情况下，系统剩余可用内存为104G，计算containers值如下： containers = min (232, 1.8 7 , (128-24)/2) = min (64, 12.6 , 51) = 13 计算RAM-per-container值如下： RAM-per-container = max (2, (124-24)/13) = max (2, 8) = 8 你也可以使用脚本yarn-utils.py来计算上面的值： #!/usr/bin/env python import optparse from pprint import pprint import logging import sys import math import ast &apos;&apos;&apos; Reserved for OS + DN + NM, Map: Memory =&gt; Reservation &apos;&apos;&apos; reservedStack = { 4:1, 8:2, 16:2, 24:4, 48:6, 64:8, 72:8, 96:12, 128:24, 256:32, 512:64} &apos;&apos;&apos; Reserved for HBase. Map: Memory =&gt; Reservation &apos;&apos;&apos; reservedHBase = {4:1, 8:1, 16:2, 24:4, 48:8, 64:8, 72:8, 96:16, 128:24, 256:32, 512:64} GB = 1024 def getMinContainerSize(memory): if (memory &lt;= 4): return 256 elif (memory &lt;= 8): return 512 elif (memory &lt;= 24): return 1024 else: return 2048 pass def getReservedStackMemory(memory): if (reservedStack.has_key(memory)): return reservedStack[memory] if (memory &lt;= 4): ret = 1 elif (memory &gt;= 512): ret = 64 else: ret = 1 return ret def getReservedHBaseMem(memory): if (reservedHBase.has_key(memory)): return reservedHBase[memory] if (memory &lt;= 4): ret = 1 elif (memory &gt;= 512): ret = 64 else: ret = 2 return ret def main(): log = logging.getLogger(__name__) out_hdlr = logging.StreamHandler(sys.stdout) out_hdlr.setFormatter(logging.Formatter(&apos; %(message)s&apos;)) out_hdlr.setLevel(logging.INFO) log.addHandler(out_hdlr) log.setLevel(logging.INFO) parser = optparse.OptionParser() memory = 0 cores = 0 disks = 0 hbaseEnabled = True parser.add_option(&apos;-c&apos;, &apos;--cores&apos;, default = 16, help = &apos;Number of cores on each host&apos;) parser.add_option(&apos;-m&apos;, &apos;--memory&apos;, default = 64, help = &apos;Amount of Memory on each host in GB&apos;) parser.add_option(&apos;-d&apos;, &apos;--disks&apos;, default = 4, help = &apos;Number of disks on each host&apos;) parser.add_option(&apos;-k&apos;, &apos;--hbase&apos;, default = &quot;True&quot;, help = &apos;True if HBase is installed, False is not&apos;) (options, args) = parser.parse_args() cores = int (options.cores) memory = int (options.memory) disks = int (options.disks) hbaseEnabled = ast.literal_eval(options.hbase) log.info(&quot;Using cores=&quot; + str(cores) + &quot; memory=&quot; + str(memory) + &quot;GB&quot; + &quot; disks=&quot; + str(disks) + &quot; hbase=&quot; + str(hbaseEnabled)) minContainerSize = getMinContainerSize(memory) reservedStackMemory = getReservedStackMemory(memory) reservedHBaseMemory = 0 if (hbaseEnabled): reservedHBaseMemory = getReservedHBaseMem(memory) reservedMem = reservedStackMemory + reservedHBaseMemory usableMem = memory - reservedMem memory -= (reservedMem) if (memory &lt; 2): memory = 2 reservedMem = max(0, memory - reservedMem) memory *= GB containers = int (min(2 * cores, min(math.ceil(1.8 * float(disks)), memory/minContainerSize))) if (containers &lt;= 2): containers = 3 log.info(&quot;Profile: cores=&quot; + str(cores) + &quot; memory=&quot; + str(memory) + &quot;MB&quot; + &quot; reserved=&quot; + str(reservedMem) + &quot;GB&quot; + &quot; usableMem=&quot; + str(usableMem) + &quot;GB&quot; + &quot; disks=&quot; + str(disks)) container_ram = abs(memory/containers) if (container_ram &gt; GB): container_ram = int(math.floor(container_ram / 512)) * 512 log.info(&quot;Num Container=&quot; + str(containers)) log.info(&quot;Container Ram=&quot; + str(container_ram) + &quot;MB&quot;) log.info(&quot;Used Ram=&quot; + str(int (containers*container_ram/float(GB))) + &quot;GB&quot;) log.info(&quot;Unused Ram=&quot; + str(reservedMem) + &quot;GB&quot;) log.info(&quot;yarn.scheduler.minimum-allocation-mb=&quot; + str(container_ram)) log.info(&quot;yarn.scheduler.maximum-allocation-mb=&quot; + str(containers*container_ram)) log.info(&quot;yarn.nodemanager.resource.memory-mb=&quot; + str(containers*container_ram)) map_memory = container_ram reduce_memory = 2*container_ram if (container_ram &lt;= 2048) else container_ram am_memory = max(map_memory, reduce_memory) log.info(&quot;mapreduce.map.memory.mb=&quot; + str(map_memory)) log.info(&quot;mapreduce.map.java.opts=-Xmx&quot; + str(int(0.8 * map_memory)) +&quot;m&quot;) log.info(&quot;mapreduce.reduce.memory.mb=&quot; + str(reduce_memory)) log.info(&quot;mapreduce.reduce.java.opts=-Xmx&quot; + str(int(0.8 * reduce_memory)) + &quot;m&quot;) log.info(&quot;yarn.app.mapreduce.am.resource.mb=&quot; + str(am_memory)) log.info(&quot;yarn.app.mapreduce.am.command-opts=-Xmx&quot; + str(int(0.8*am_memory)) + &quot;m&quot;) log.info(&quot;mapreduce.task.io.sort.mb=&quot; + str(int(0.4 * map_memory))) pass if __name__ == &apos;__main__&apos;: try: main() except(KeyboardInterrupt, EOFError): print(&quot;\\nAborting ... Keyboard Interrupt.&quot;) sys.exit(1)执行下面命令： python yarn-utils.py -c 32 -m 128 -d 7 -k False 返回结果如下： Using cores=32 memory=128GB disks=7 hbase=False Profile: cores=32 memory=106496MB reserved=24GB usableMem=104GB disks=7 Num Container=13 Container Ram=8192MB Used Ram=104GB Unused Ram=24GB yarn.scheduler.minimum-allocation-mb=8192 yarn.scheduler.maximum-allocation-mb=106496 yarn.nodemanager.resource.memory-mb=106496 mapreduce.map.memory.mb=8192 mapreduce.map.java.opts=-Xmx6553m mapreduce.reduce.memory.mb=8192 mapreduce.reduce.java.opts=-Xmx6553m yarn.app.mapreduce.am.resource.mb=8192 yarn.app.mapreduce.am.command-opts=-Xmx6553m mapreduce.task.io.sort.mb=3276这样的话，每个container内存为8G，似乎有点多，我更愿意根据集群使用情况任务将其调整为2G内存，则集群中下面的参数配置值如下：配置文件 配置设置 计算值yarn-site.xml yarn.nodemanager.resource.memory-mb = 52 * 2 =104 Gyarn-site.xml yarn.scheduler.minimum-allocation-mb = 2Gyarn-site.xml yarn.scheduler.maximum-allocation-mb = 52 * 2 = 104Gyarn-site.xml (check) yarn.app.mapreduce.am.resource.mb = 2 * 2=4Gyarn-site.xml (check) yarn.app.mapreduce.am.command-opts = 0.8 * 2 * 2=3.2Gmapred-site.xml mapreduce.map.memory.mb = 2Gmapred-site.xml mapreduce.reduce.memory.mb = 2 * 2=4Gmapred-site.xml mapreduce.map.java.opts = 0.8 * 2=1.6Gmapred-site.xml mapreduce.reduce.java.opts = 0.8 * 2 * 2=3.2G 对应的xml配置为： &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;106496&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;106496&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;-Xmx3276m&lt;/value&gt; &lt;/property&gt;另外，还有一下几个参数： yarn.nodemanager.vmem-pmem-ratio：任务每使用1MB物理内存，最多可使用虚拟内存量，默认是2.1。 yarn.nodemanager.pmem-check-enabled：是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 yarn.nodemanager.vmem-pmem-ratio：是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。第一个参数的意思是当一个map任务总共分配的物理内存为2G的时候，该任务的container最多内分配的堆内存为1.6G，可以分配的虚拟内存上限为2*2.1=4.2G。另外，照这样算下去，每个节点上YARN可以启动的Map数为104/2=52个。CPU配置 YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟CPU个数。 在YARN中，CPU相关配置参数如下： yarn.nodemanager.resource.cpu-vcores：表示该节点上YARN可使用的虚拟CPU个数，默认是8，注意，目前推荐将该值设值为与物理CPU核数数目相同。如果你的节点CPU核数不够8个，则需要调减小这个值，而YARN不会智能的探测节点的物理CPU总数。 yarn.scheduler.minimum-allocation-vcores：单个任务可申请的最小虚拟CPU个数，默认是1，如果一个任务申请的CPU个数少于该数，则该对应的值改为这个数。 yarn.scheduler.maximum-allocation-vcores：单个任务可申请的最多虚拟CPU个数，默认是32。对于一个CPU核数较多的集群来说，上面的默认配置显然是不合适的，在我的测试集群中，4个节点每个机器CPU核数为31，留一个给操作系统，可以配置为： &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;31&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;124&lt;/value&gt; &lt;/property&gt;","link":"/2020/03/03/YARN%E7%9A%84Memory%E5%92%8CCPU%E8%B0%83%E4%BC%98%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/"},{"title":"Wordcount案例的shuffle","text":"一、流程介绍map到reduce过程:map—shuffle—-&gt;reduce19/07/13 19:55:31 INFO mapreduce.Job: map 0% reduce 0%19/07/13 19:55:42 INFO mapreduce.Job: map 50% reduce 0%19/07/13 19:55:43 INFO mapreduce.Job: map 100% reduce 0%19/07/13 19:55:50 INFO mapreduce.Job: map 100% reduce 100% shuffle相同的key通过网络传输 拉到一起，当某个key的数据量特别大，会有数据倾斜。1、提交文件到mapreduce，假如文件大小为260M，但块的默认大小为128M,就会有三个块2、文件会被进行splitting，将文件分割成三个块，大多数split分割都是按照块进行的。3、被分割的块文件进行map映射，启动的task数量为34、然后通过网络传输key至suffling环节,相同的key拉到一起，当某个key的数据量特别大，会有数据倾斜。（suffling可以认为是处于reducing阶段，行业内说法）5、reduce就是将之前的数据进行一个整合，最终输出到一个文件中，默认task数量为1 ， 参数：mapreduce.job.reduces","link":"/2017/12/02/Wordcount%E6%A1%88%E4%BE%8B%E7%9A%84shuffle/"},{"title":"yarn的调度器和常用命令","text":"一、调度器yarn的 三种调度器：FIFO 先进先出capacity 计算Fair 公平 生产大部分使用的是公平调度器 1.FIFO先进先出，一个的简单调度器，适合低负载集群。（适合任务数量不多的情况下使用）。示例：1job先提交，运行完成后，2job开始提交运行 2 .Capacity调度器，给不同队列（即用户或用户组）分配一个预期最小容量，在每个队列内部用层次化的FIFO来调度多个应用程序。（适用于有很多小的任务跑，需要占很多队列，不使用队列，会造成资源的浪费）。示例：1job提交运行，当2job提交后，会在另一个队列进行2job的计算，1job和2job可以同时进行。 3.Fair公平调度器，针对不同的应用（也可以为用户或用户组），每个应用属于一个队列，主旨是让每个应用分配的资源大体相当。（当然可以设置权重），若是只有一个应用，那集群所有资源都是他的。 适用情况：共享大集群、队列之间有较大差别。（生产使用）示例：1job先提交运行，2job也提交运行，从2job提交到获得资源会有一定的延迟，因为它需要等待1job释放占用的Container。当2job执行完成之后也会释放自己占用的资源，1job又获得了全部的系统资源。 二、常用命令mapred常用命令:mapred jobmapred job -list 1234#表示杀死正在运行的job[-kill &lt;job-id&gt;]#查看正在运行的job[-list [all]] yarn的常用命令： -list -kill 123456-kill &lt;Application ID&gt; Kills the application. CDH Web -list List applications. Supports optional use of -appTypes to filter applications based on application type, and -appStates to filter applications based on application state. 注：kill要谨慎使用，在kill之前要进行确认是否确认需要kill","link":"/2020/03/03/Yarn%E7%9A%84%E8%B0%83%E5%BA%A6%E5%99%A8%E5%92%8C%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"title":"Yarn和MapReduce的内存优化配置详解","text":"在Hadoop2.x中, YARN负责管理MapReduce中的资源(内存, CPU等)并且将其打包成Container。使之专注于其擅长的数据处理任务, 将无需考虑资源调度. 如下图所示 YARN会管理集群中所有机器的可用计算资源. 基于这些资源YARN会调度应用(比如MapReduce)发来的资源请求, 然后YARN会通过分配Container来给每个应用提供处理能力, Container是YARN中处理能力的基本单元, 是对内存, CPU等的封装. 目前我这里的服务器情况：6台slave，每台：32G内存，2*6核CPU。 由于hadoop 1.x存在JobTracker和TaskTracker，资源管理有它们实现，在执行mapreduce作业时，资源分为map task和reduce task。所有存在下面两个参数分别设置每个TaskTracker可以运行的任务数： &lt;property&gt; &lt;name&gt;mapred.tasktracker.map.tasks.maximum&lt;/name&gt; &lt;value&gt;6&lt;/value&gt; &lt;description&gt;&lt;![CDATA[CPU数量=服务器CPU总核数 / 每个CPU的核数；服务器CPU总核数 = more /proc/cpuinfo | grep &apos;processor&apos; | wc -l；每个CPU的核数 = more /proc/cpui nfo | grep &apos;cpu cores&apos;]]&gt;&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.tasktracker.reduce.tasks.maximum&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;description&gt;一个task tracker最多可以同时运行的reduce任务数量&lt;/description&gt; &lt;/property&gt;但是在hadoop 2.x中，引入了Yarn架构做资源管理，在每个节点上面运行NodeManager负责节点资源的分配，而slot也不再像1.x那样区分Map slot和Reduce slot。在Yarn上面Container是资源的分配的最小单元。 Yarn集群的内存分配配置在yarn-site.xml文件中配置： &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;22528&lt;/value&gt; &lt;discription&gt;每个节点可用内存,单位MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1500&lt;/value&gt; &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;16384&lt;/value&gt; &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt; &lt;/property&gt;由于我Yarn集群还需要跑Spark的任务，而Spark的Worker内存相对需要大些，所以需要调大单个任务的最大内存（默认为8G）。 而Mapreduce的任务的内存配置： &lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;1500&lt;/value&gt; &lt;description&gt;每个Map任务的物理内存限制&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;3000&lt;/value&gt; &lt;description&gt;每个Reduce任务的物理内存限制&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt;-Xmx1200m&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt;-Xmx2600m&lt;/value&gt; &lt;/property&gt;mapreduce.map.memory.mb：每个map任务的内存，应该是大于或者等于Container的最小内存。按照上面的配置：每个slave可以运行map的数据&lt;= 22528/1500,reduce任务的数量&lt;=22528/3000 。 mapreduce.map.memory.mb &gt;mapreduce.map.java.optsmapreduce.reduce.memory.mb &gt;mapreduce.reduce.java.opts mapreduce.map.java.opts / mapreduce.map.memory.mb=0.700.80mapreduce.reduce.java.opts / mapreduce.reduce.memory.mb=0.700.80 在yarn container这种模式下，JVM进程跑在container中，mapreduce.{map|reduce}.java.opts 能够通过Xmx设置JVM最大的heap的使用，一般设置为0.75倍的memory.mb， 则预留些空间会存储java,scala code等。","link":"/2020/03/03/Yarn%E5%92%8CMapReduce%E7%9A%84%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/"},{"title":"Yarn资源调优参数及案例","text":"1.yarn内存调优 说明：yarn主件的nodemanager进程管理的resource的memory资源可以分配给容器的物理内存(以MB为单位)。如果设置为-1，并yarn.nodemanager.resource.detect-hardware- capability为真，则自动计算(在Windows和Linux中)。在其他情况下，默认值是8192MB。yarn.nodemanager.resource.memory-mbyarn.scheduler.minimum-allocation-mb 分配的最小内存大小默认为1024MByarn.scheduler.maximum-allocation-mb 分配的最大内存大小默认为8192MB 2.yarnCPU调优 说明：Vcores ,虚拟的core，是yarn提出的物理core:vcore=1:2可以分配给容器的vcore数量。这是RM调度程序在为容器分配资源时使用的。这并不用于限制yarn容器使用的cpu数量。如果将其设置为-1，并且yarn.nodemanager.resource.detect-hardware- capability为真，则在Windows和Linux中，将从硬件自动确定它。在其他情况下，vcore的数量默认为8。yarn.nodemanager.resource.cpu-vcoresyarn.scheduler.minimum-allocation-vcores 分配给容器的最小vcore数量为1yarn.scheduler.maximum-allocation-vcores 分配给容器的最大vcore数量为4 3案例机器的物理内存大小为64G 16cores 怎样资源最大化利用 一般会分配2G给各系统，实际可用内存为62G，系统不会消耗CUP，所以CPU的核数还是16c。通常会预留15%-20%的内存，以防内存消耗完，会触发OOM机制。62*80%=50G给计算资源剩余12G, 通常分配4G给我们的datanode，分配1G给我们的nodemanager， 剩余7G计算时会启动Container容器，启动额外的进程需要一定的内存 剩余的50G，可以这样分配。 yarn.nodemanager.resource.memory-mb : 50G 计算总内存yarn.scheduler.minimum-allocation-mb : 1Gincrementyarn.scheduler.maximum-allocation-mb : 1G–&gt;50G#所以Container的个数范围：1~50#DN memory:4G#Nm memory:1G#有16c–&gt;会有32vcore，尽量不要超出32vcore yarn.nodemanager.resource.memory-mb 50Gyarn.nodemanager.resource.cpu-vcores 32c 我们将一核虚拟化成2个vcoreyarn.scheduler.minimum-allocation-mb 2G 根据业务设置yarn.scheduler.minimum-allocation-vcores 2c 根据业务设置，根据需要递增的yarn.scheduler.maximum-allocation-mb 8G 根据业务设置yarn.scheduler.maximum-allocation-vcores 4c 这个值可以确定，经过clouder公司测试，vcores在小于5的时候，性能是最佳的 注：内存不够的时候会自动累加，Vcore不够的时候不会自动累加，需要在启动的时候设置","link":"/2020/03/03/Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98%E5%8F%82%E6%95%B0%E5%8F%8A%E6%A1%88%E4%BE%8B/"},{"title":"hdfs读流程和写流程","text":"一.文件读流程 1、Client通过FileSystem.open(filePath)方法,与NN节点进行【rpc】协议通信，NN接受到请求会校验是否有权限是否存在，假如都ok，返回该文件的部分或全部的block的列表(包含各个block块的分布在DN地址的列表)，也就是返回【FSDataInputStream】对象；如果过不了check直接抛出异常。 2、Clinet调用FSDataInputStream.read方法，a.与第一个块的最近的DN进行read，读取完成后，会check,假如ok，会关闭与当前的DN的通信，假如失败会记录块的这个副本+DN信息，下次就不会从这读取。那么就去该块的第二个DN的地址读取b.如果没问题就继续读取下一个blockc.假如block列表读取完成后，文件还未结束，那么FileSystem会从NN获取下一批次的block的列表 3.Client调用FSDataInputStream.close()方法，关闭输入流 一.文件写流程)1.Client调用FileSystem.create(filePath)方法,与NN进行【rpc】通信，检验该路径是否有权限创建是否文件存在，假如ok，就创建一个新的文件，但不关联任何的block，返回一个【FSDataOutputStream】（假如不ok，直接返回错误，代码加try catch） 2.Client调用FSDataOutputStream.write方法a.将第一个块的第一个副本写入DN1,第一个副本写完传输给第二个DN2,第二个副本写完就传输给第三个DN3，当第DN3写完，就返回一个ack packet给DN2，DN2就返回ack packet给DN1，DN1就返回ack packet的FSDataOutputStream对象，标识第一个块的三个副本都写完了 b.余下的块依次这样 3 当向文件写入数据完成后，Client调用FSDataOutputStream.close()方法关闭输出流 4 再调用FileSystem.complete()方法，告诉NN节点写入成功","link":"/2020/03/03/hdfs%E8%AF%BB%E6%B5%81%E7%A8%8B%E5%92%8C%E5%86%99%E6%B5%81%E7%A8%8B/"},{"title":"hadoop-2.6.0-cdh5.7.0源码编译","text":"title：hadoop-2.6.0-cdh5.7.0源码编译一、说明直接使用的hadoop-2.6.0-cdh5.7.0.tar.gz包部署的hadoop集群不支持文件压缩，生产上是不可接受的，故需要将hadoop源码下载重新编译支持压缩 二、编译hadoop支持压缩 1、编译流程：下载软件——&gt;安装必要依赖库——&gt;添加用户并创建文件夹上传软件——&gt;安装JDK并添加环境变量——&gt;安装maven并配置和添加环境变量——&gt;安装protobuf并添加环境变量和编译——&gt;编译hadoop支持压缩 2、步骤解读： （1）软件下载 组件版本 百度网盘链接 Hadoop-2.6.0-cdh5.7.0-src.tar.gz https://pan.baidu.com/s/1uRMGIhLSL9QHT-Ee4F16jw 提取码：jb1d jdk-7u80-linux-x64.tar.gz https://pan.baidu.com/s/1xSCQ8rjABVI-zDFQS5nCPA 提取码：lfze apache-maven-3.3.9-bin.tar.gz https://pan.baidu.com/s/1ddkdkLW7r7ahFZmgACGkVw 提取码：fdfz protobuf-2.5.0.tar.gz https://pan.baidu.com/s/1RSNZGd_ThwknMB3vDkEfhQ 提取码：hvc2 注：编译的JDK版本必须是1.7，1.8的JDK会导致编译失败 （2）安转必要依赖库 1234[root@hadoop001 ~]# yum install -y svn ncurses-devel[root@hadoop001 ~]# yum install -y gcc gcc-c++ make cmake[root@hadoop001 ~]# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool[root@hadoop001 ~]# yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake cmake (3)添加用户并创建文件夹上传软件 12345678910111213[root@hadoop001 ~]# yum install -y lrzsz[root@hadoop001 ~]# useradd hadoop[root@hadoop001 ~]# su - hadoop[hadoop@hadoop001 ~]$ mkdir app soft source lib data maven_repo shell mysql[hadoop@hadoop001 ~]$ cd soft/[hadoop@hadoop001 soft]$ rz上传后 ll 查看一下是否有误[hadoop@hadoop001 soft]$ lltotal 202192-rw-r--r--. 1 hadoop hadoop 8491533 Apr 7 11:25 apache-maven-3.3.9-bin.tar.gz-rw-r--r--. 1 hadoop hadoop 42610549 Apr 6 16:55 hadoop-2.6.0-cdh5.7.0-src.tar.gz-rw-r--r--. 1 hadoop hadoop 153530841 Apr 7 11:12 jdk-7u80-linux-x64.tar.gz-rw-r--r--. 1 hadoop hadoop 2401901 Apr 7 11:31 protobuf-2.5.0.tar.gz （4）安装JDK并添加环境变量 解压并将文件放在/usr/java目录,修改拥有者为root 123456789101112131415161718创建/usr/java/目录，修改chown权限为root[root@hadoop001 ~]# mkdir /usr/java/[root@hadoop001 ~]# tar -zxvf /home/hadoop/soft/jdk-7u80-linux-x64.tar.gz -C /usr/java[root@hadoop001 ~]# cd /usr/java/[root@hadoop001 java]# chown -R root:root jdk1.7.0_80配置/etc/profile环境变量[root@hadoop001 jdk1.7.0_80]# vi /etc/profile 添加如下两行环境变量export JAVA_HOME=/usr/java/jdk1.7.0_80export PATH=$JAVA_HOME/bin/:$PATH[root@hadoop001 java]# source /etc/profile测试java是否安装成功[root@hadoop001 jdk1.7.0_80]# java -version java version &quot;1.7.0_80&quot; Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) （5）安装maven并配置和添加环境变量 123456789101112131415161718192021222324252627282930[root@hadoop001 ~]# su - hadoop[hadoop@hadoop001 ~]$ tar -zxvf ~/soft/apache-maven-3.3.9-bin.tar.gz -C ~/app/修改haoop用户的环境变量[hadoop@hadoop001 ~]$ vi ~/.bash_profile添加或修改如下内容，注意MAVEN_OPTS设置了maven运行的内存，防止内存太小导致编译失败export MAVEN_HOME=/home/hadoop/app/apache-maven-3.3.9export MAVEN_OPTS=&quot;-Xms1024m -Xmx1024m&quot;export PATH=$MAVEN_HOME/bin:$PATH[hadoop@hadoop001 ~]$ source ~/.bash_profile[hadoop@hadoop001 ~]$ which mvn~/app/apache-maven-3.3.9/bin/mvn[hadoop@hadoop001 protobuf-2.5.0]$ vi ~/app/apache-maven-3.3.9/conf/settings.xml配置maven的本地仓库位置注意：本地仓库位置不要放错&lt;localRepository&gt;/home/hadoop/maven_repo/repo&lt;/localRepository&gt;添加阿里云中央仓库地址注意一定要写在&lt;mirrors&gt;&lt;/mirrors&gt;之间&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 通常依赖的jar包下载不齐全，可将repo文件上传 1234567#jar包链接链接：https://pan.baidu.com/s/1vq4iVFqqyJNkYzg90bVrfg 提取码：vugv 复制这段内容后打开百度网盘手机App，操作更方便哦#下载后 rz上传解压，注意目录层次[hadoop@hadoop001 maven_repo]$ rz[hadoop@hadoop001 maven_repo]$ tar -zxvf repo.tar.gz （6）安装protobuf并添加环境变量和编译 123456789101112131415161718[hadoop@hadoop001 ~]$ tar -zxvf ~/soft/protobuf-2.5.0.tar.gz -C ~/app/[hadoop@hadoop001 protobuf-2.5.0]$ cd ~/app/protobuf-2.5.0/ --prefix= 是用来待会编译好的包放在为路径[hadoop@hadoop001 protobuf-2.5.0]$ ./configure --prefix=/home/hadoop/app/protobuf-2.5.0编译以及安装[hadoop@hadoop001 protobuf-2.5.0]$ make[hadoop@hadoop001 protobuf-2.5.0]$ make install[hadoop@hadoop001 protobuf-2.5.0]$ vim ~/.bash_profile追加如下两行内容，未编译前是没有bin目录的export PROTOBUF_HOME=/home/hadoop/app/protobuf-2.5.0export PATH=$PROTOBUF_HOME/bin:$PATH[hadoop@hadoop001 protobuf-2.5.0]$ source ~/.bash_profile 测试是否生效，若出现libprotoc 2.5.0则为生效[hadoop@hadoop001 protobuf-2.5.0]$ protoc --versionlibprotoc 2.5.0 （7）编译hadoop支持压缩 12345[hadoop@hadoop001 protobuf-2.5.0]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0-src.tar.gz -C ~/source/#进入hadoop的源码目录[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cd ~/source/hadoop-2.6.0-cdh5.7.0/#进行编译[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ mvn clean package -Pdist,native -DskipTests -Dtar 查看编译后的包：hadoop-2.6.0-cdh5.7.0.tar.gz 123456789101112131415161718192021222324252627282930313233#有 BUILD SUCCESS 信息则表示编译成功[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 13.592 s][INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.042 s][INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.094 s][INFO] Apache Hadoop Distribution ......................... SUCCESS [01:49 min][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 37:39 min[INFO] Finished at: 2019-04-07T16:48:42+08:00[INFO] Final Memory: 200M/989M[INFO] ------------------------------------------------------------------------[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ ll /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/total 564036drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:46 antrundrwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 classes-rw-rw-r--. 1 hadoop hadoop 1998 Apr 7 16:46 dist-layout-stitching.sh-rw-rw-r--. 1 hadoop hadoop 690 Apr 7 16:47 dist-tar-stitching.shdrwxrwxr-x. 9 hadoop hadoop 4096 Apr 7 16:47 hadoop-2.6.0-cdh5.7.0-rw-rw-r--. 1 hadoop hadoop 191880143 Apr 7 16:47 hadoop-2.6.0-cdh5.7.0.tar.gz-rw-rw-r--. 1 hadoop hadoop 7314 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0.jar-rw-rw-r--. 1 hadoop hadoop 385618309 Apr 7 16:48 hadoop-dist-2.6.0-cdh5.7.0-javadoc.jar-rw-rw-r--. 1 hadoop hadoop 4855 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0-sources.jar-rw-rw-r--. 1 hadoop hadoop 4855 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0-test-sources.jardrwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:47 javadoc-bundle-optionsdrwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:47 maven-archiverdrwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 maven-shared-archive-resourcesdrwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 test-classesdrwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:46 test-dir 三、举例说明编译错误以及处理方式 （1）第一种情况 1[WARNING] Unrecognised tag: 'mirror' (position: START_TAG seen ...&lt;/mirrors&gt;\\n&lt;mirror&gt;... @163:9) @ /home/hadoop/app/apache-maven-3.3.9/conf/settings.xml, line 163, column 9 原因：maven配置错误，未将阿里云中央仓库地址写在 &lt; mirrors&gt; &lt; /mirrors &gt; 之间 （2）第二种情况： 123[FATAL] Non-resolvable parent POM for org.apache.hadoop:hadoop-main:2.6.0-cdh5.7.0: Could not transfer artifact com.cloudera.cdh:cdh-root:pom:5.7.0 from/to cdh.repo (https://repository.cloudera.com/artifactory/cloudera-repos): Remote host closed connectio#分析：是https://repository.cloudera.com/artifactory/cloudera-repos/com/cloudera/cdh/cdh-root/5.7.0/cdh-root-5.7.0.pom文件下载不了#解决方案：前往本地仓库到目标文件目录，然后 通过wget 文件，来成功获取该文件，重新执行编译命令，或者执行4.5的可选步骤，将需要的jar直接放到本地仓库 文件cdh-root-5.7.0.pom未下载下来，解决方式将该文件手动下载上传，pom文件位置（com.cloudera.cdh:cdh-root:pom:5.7.0），打开这个网址https://repository.cloudera.com/artifactory/cloudera-repos/找到对应的pom文件下载后放在maven仓库的文件夹maven仓库的文件夹位置：在/home/hadoop/maven_repo/repo目录下查看对应的是缺少哪个目录下的pom和jar包文件重新执行 mvn clean package -Pdist,native -DskipTests -Dtar 1234567举例说明pom文件位置[hadoop@hadoop001 1.7.6-cdh5.7.0]$ pwd/home/hadoop/maven_repo/repo/org/apache/avro/avro-compiler/1.7.6-cdh5.7.0[hadoop@hadoop001 1.7.6-cdh5.7.0]$ lltotal 152-rw-r--r--. 1 hadoop hadoop 77422 Dec 2 2018 avro-compiler-1.7.6-cdh5.7.0.jar-rw-r--r--. 1 hadoop hadoop 4690 Dec 2 2018 avro-compiler-1.7.6-cdh5.7.0.pom","link":"/2020/03/03/hadoop-2.6.0-cdh5.7.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"},{"title":"hive之DML和DDL语句","text":"一、数据库内容1、创建数据库如果重复创建会提示存在，添加 IF NOT EXISTS 则不存在问题location默认的是在hdfs上use/werehost上面，如果自己指定的是不带DB的，需要创建 官网语法： 1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name[COMMENT database_comment][LOCATION hdfs_path][WITH DBPROPERTIES (property_name=property_value, ...)]; 12345678910111213141516171819202122232425262728293031323334353637示例：hive (default)&gt; create database d7_hive;OKTime taken: 12.868 secondshive (default)&gt; show databases;OKdatabase_named7_hivedefaultTime taken: 0.576 seconds, Fetched: 2 row(s)如果重复创建会提示存在，添加 IF NOT EXISTS 则不存在问题hive (default)&gt; reate database d7_hive;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database ruozedata already existshive (default)&gt; create database if not exists d7_hive;OKTime taken: 0.017 seconds不指定创建数据库路径，默认创建在/user/hive/warehouse目录下，有一个数据库名.db的文件夹[hadoop@hadoop001 ~]$ hdfs dfs -ls /user/hive/warehouseFound 2 itemsdrwxr-xr-x - hadoop supergroup 0 2019-07-28 00:02 /user/hive/warehouse/d7_hive.dbdrwxr-xr-x - hadoop supergroup 0 2019-07-27 18:02 /user/hive/warehouse/hello指定创建数据库目录，自己指定的是不带DB的，需要创建，创建好之后对应的文件夹下面会有对应的文件需要注意的创建表默认是内部表，指定EXTERNAL则是外部表，两者的区别在于删除时内部表hdfs上数据和元数据都会被删除，外部表只删除元数据不会。hive (default)&gt; use d7_hive;OKTime taken: 0.153 secondshive (d7_hive)&gt; create table hello(id int,name string) &gt; row format delimited fields terminated by ',';OKTime taken: 2.172 secondshive (d7_hive)&gt; &gt; load data local inpath '/home/hadoop/data/hello.txt' overwrite into table hello;Loading data to table d7_hive.helloTable d7_hive.hello stats: [numFiles=1, totalSize=28]OKTime taken: 3.102 seconds 2、删除数据库数据库删除的时候，只要库下有表，默认就不能删除如果必须删除，在语句后加入cascadeCASCADE慎用 官网语法： 1DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; 1234删除的时候提示错误，库下存在一个或多个表hive (default)&gt; DROP DATABASE IF EXISTS d7_hive;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database d7_hive is not empty. One or more tables exist.)语句后加入cascade可删除，但要慎用 3、元数据的信息关联mysql&gt;show tables;——几个重要的表： VERSION表中有且只能存在一条记录，描述表的信息DBS databases 库TBLS tables 表COLUMNS_V2 描述表的字段 mysql&gt;select * from DBS; mysql&gt;select * from DBS \\G;每一个SQL语句底层都有相应的元数据信息做关联数据库创建完成后，元数据信息在DBS表里面 二、表的内容 字段的数据类型*数值：int、bigint、float、double*字符串：string 至少90%会用到时间类型可以使用字符串类型： stringBOOLEAN：true false 用0/1会更简单 分隔符字段和字段分隔符、行与行的分隔符字段和字段分隔符：hive里面默认的分隔符 \\001 ^A ，生产上最多的是， 逗号、空格、制表符tab行与行的分隔符 ：\\n 举例：1,ruoze2,jepson3,xingxing==&gt; row format delimited fields terminated by ‘,’; 1、创建表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778官方语法CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables) CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; data_type : primitive_type | array_type | map_type | struct_type | union_type -- (Note: Available in Hive 0.7.0 and later) primitive_type : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN | FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later) array_type : ARRAY &lt; data_type &gt; map_type : MAP &lt; primitive_type, data_type &gt; struct_type : STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt; union_type : UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later) row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] file_format: : SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | JSONFILE -- (Note: Available in Hive 4.0.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname constraint_specification: : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE 1234567891011121314151617181920212223创建表hive (default)&gt; create table emp( &gt; empno int, &gt; ename string, &gt; job string, &gt; mgr int, &gt; hiredate string, &gt; sal double, &gt; comm double, &gt; deptno int) &gt; row format delimited fields terminated by ',';OKTime taken: 0.083 secondsCreate Table As Select (CTAS)--------------------- 重新拷贝一个表CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_nameLIKE existing_table_or_view_name[LOCATION hdfs_path];拷贝表结构，不拷贝表数据CREATE TABLE ruozedata_emp2 LIKE ruozedata_emp; 如果不知道表结构，通过show create table ruozedata_emp;查看 123真正拷贝一份数据Create Table As Select (CTAS)create table ruozedata_emp3 as select empno,ename,deptno from ruozedata_emp; 123修改表名ALTER TABLE 老表 RENAME TO 新表;ALTER TABLE ruozedata_emp3 rename to ruozedata_emp3_bak; 1234删除表DROP TABLE [IF EXISTS] table_name [PURGE]; DROP TABLE table_name ruozedata_emp3_bak;Truncate Table 删表的数据","link":"/2020/03/03/hive%E4%B9%8BDML%E5%92%8CDDL%E8%AF%AD%E5%8F%A5/"},{"title":"jps命令使用","text":"jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。可以通过它来查看我们到底启动了几个java进程 which查看jps的文件位置如果环境里面没有jps说明JDK未配置好如果在当前命令行下打 jps，需要配置环境变量将JDK设置为全局变量 1234567[hadoop@hadoop001 hadoop]$ which jps/usr/java/jdk1.8.0_45/bin/jps[hadoop@hadoop001 hadoop]$ jps3778 SecondaryNameNode4503 Jps3500 NameNode3597 DataNode jps存储位置是在hsperfdata_hadoop目录下 1234567[hadoop@hadoop001 ~]$ cd /tmp[hadoop@hadoop001 tmp]$ cd hsperfdata_hadoop/[hadoop@hadoop001 hsperfdata_hadoop]$ lltotal 96-rw-------. 1 hadoop hadoop 32768 Jul 4 22:10 5010-rw-------. 1 hadoop hadoop 32768 Jul 4 22:10 5107-rw-------. 1 hadoop hadoop 32768 Jul 4 22:10 5296 jps注意事项 12345678#进程所在的用户jps查看会显示，#非root用户无进程显示#root显示process information unavailable[root@hadoop001 ~]# jps5296 -- process information unavailable5537 Jps5010 -- process information unavailable5107 -- process information unavailable 假如kill一个进程——还会有进程存在，这个是属于一种假象，会停留一会时间kill有时候会自动kill碰见这句 process information unavailable有可能可用 有可能不可用 –》 ps -ef|grep xxx是否实际存在就行 12345678910[root@hadoop001 ~]# kill -9 4920[root@hadoop001 ~]# jps4920 -- process information unavailable5051 -- process information unavailable5468 Jps[hadoop@hadoop001 hsperfdata_hadoop]$ jps5480 Jps5051 SecondaryNameNode[hadoop@hadoop001 hsperfdata_hadoop]$ pid文件 进程启动 停止的所需的文件 123456[hadoop@hadoop001 tmp]$ ll#pid文件位置-rw-rw-r--. 1 hadoop hadoop 5 Jul 4 22:02 hadoop-hadoop-datanode.pid-rw-rw-r--. 1 hadoop hadoop 5 Jul 4 22:02 hadoop-hadoop-namenode.pid-rw-rw-r--. 1 hadoop hadoop 5 Jul 4 22:02 hadoop-hadoop-secondarynamenode. 生产上 一般会把重要的配置路径 存放到另外的目录下[root@hadoop001 log]# mkdir /data/tmp[root@hadoop001 log]# chmod -R 777 /data/tmp[root@hadoop001 log]#","link":"/2020/03/03/jps%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/02/29/hello-world/"},{"title":"shell脚本同步集群配置以及zk未启动的排查方法","text":"一、shell脚本同步集群配置说明维护集群不可能每台机器都通过win上传文件，通过写脚本，以第一台为标准进行配置，下发到所有机器，除非有些参数需要动态去修改1、需要的shell脚本，给脚本一个x的权限 链接：https://pan.baidu.com/s/1_GsD--aJtQjSsjKINzBfRA提取码：l2i7 2、使用scp适用于机器少的情况使用scp的情况适用于机器少的时候，如果集群有1000或者更多的话，写scp要写1000行不现实，可以写一个for循环去处理，只要机器命名的命名规则是一样的，后面的001/002/003都是可以进行递增的另外：维护一个txt,txt文档里面维护了ruozedata001/002/003机器名称的列表，写for循环的时候可以直接获取每一行每一个机器名字 1234567891011[hadoop@ruozedata001 hadoop]$ cat sync_hadoop.sh#!/bin/bash -xHADOOP_CONF=/home/hadoop/app/hadoop/etc/hadoop/cd $HADOOP_CONFscp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml ruozedata002:$HADOOP_CONFscp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml ruozedata003:$HADOOP_CONFexit 0 1、./sync_hadoop.sh以第一台机器为版本同步xml文件和env.sh脚本到其他机器 12345678910111213141516[hadoop@ruozedata001 hadoop]$ ./sync_hadoop.sh+ HADOOP_CONF=/home/hadoop/app/hadoop/etc/hadoop/+ cd /home/hadoop/app/hadoop/etc/hadoop/+ scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml ruozedata002:/home/hadoop/app/hadoop/etc/hadoop/hadoop-env.sh 100% 4233 4.1KB/s 00:00 core-site.xml 100% 2360 2.3KB/s 00:00 hdfs-site.xml 100% 4544 4.4KB/s 00:00 mapred-site.xml 100% 1063 1.0KB/s 00:00 yarn-site.xml 100% 5334 5.2KB/s 00:00 + scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml ruozedata003:/home/hadoop/app/hadoop/etc/hadoop/hadoop-env.sh 100% 4233 4.1KB/s 00:00 core-site.xml 100% 2360 2.3KB/s 00:00 hdfs-site.xml 100% 4544 4.4KB/s 00:00 mapred-site.xml 100% 1063 1.0KB/s 00:00 yarn-site.xml 100% 5334 5.2KB/s 00:00 + exit 0 2、jps.sh 执行jps.sh，只有jps本身进程启动 123456789101112131415161718[hadoop@ruozedata001 hadoop]$ ./jps.sh+ echo '----------ruozedata001 process------------'----------ruozedata001 process------------+ ssh ruozedata001 /usr/java/jdk1.8.0_45/bin/jps2743 Jps+ echo ' ' + echo '----------ruozedata002 process------------'----------ruozedata002 process------------+ ssh ruozedata002 /usr/java/jdk1.8.0_45/bin/jps2229 Jps+ echo ' ' + echo '----------ruozedata003 process------------'----------ruozedata003 process------------+ ssh ruozedata003 /usr/java/jdk1.8.0_45/bin/jps2231 Jps+ echo ' 3、start_cluster.sh3.1 ./start_cluster.sh 执行启动集群的脚本， ./jps.sh查看启动的进程 1234567891011[hadoop@ruozedata001 hadoop]$ ./jps.sh+ echo '----------ruozedata001 process------------'----------ruozedata001 process------------+ ssh ruozedata001 /usr/java/jdk1.8.0_45/bin/jps3216 JournalNode3796 Jps2920 NameNode3740 JobHistoryServer3023 DataNode3583 NodeManagerzookeeper的进程没有启动 12[hadoop@ruozedata001 hadoop]$ ps -ef | grep zookeeperhadoop 3850 2711 0 16:47 pts/0 00:00:00 grep --color=auto zookeeper 3.2 找zk log 123456789101112131415[hadoop@ruozedata001 ~]$ cd app/zookeeperlog日志一般在配置文件里面[hadoop@ruozedata001 zookeeper]$ cd conf一般是在cfg里面[hadoop@ruozedata001 conf]$ cat zoo.cfg可以看到data的路径，没有问题dataDir=/home/hadoop/data/zookeeper查看log4j.properties[hadoop@ruozedata001 conf]$ more log4j.properties# Define some default values that can be overridden by system propertieszookeeper.root.logger=INFO, CONSOLEzookeeper.console.threshold=INFOzookeeper.log.dir=. .代表的是当前路径zookeeper.log.file=zookeeper.log 找到log日志文件是zookeeper.log 3.3找不到zookeeper.log 12345查找zookeeper.log日志文件位置，找不到zookeeper.log[hadoop@ruozedata001 conf]$ find /home/hadoop -name 'zookeeper.log' [hadoop@ruozedata001 conf]$ exitlogout[root@ruozedata001 ~]# find / -name 'zookeeper.log' 3.4 zkServer.sh 启动源头zkServer.sh start|stop|status 12345678910111213141516171819202122[hadoop@ruozedata001 bin]$ vi zkServer.shcase $1 instart) echo -n &quot;Starting zookeeper ... &quot; if [ -f &quot;$ZOOPIDFILE&quot; ]; then if kill -0 `cat &quot;$ZOOPIDFILE&quot;` &gt; /dev/null 2&gt;&amp;1; then echo $command already running as process `cat &quot;$ZOOPIDFILE&quot;`. exit 0 fi fi nohup &quot;$JAVA&quot; &quot;-Dzookeeper.log.dir=${ZOO_LOG_DIR}&quot; &quot;-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}&quot; \\ -cp &quot;$CLASSPATH&quot; $JVMFLAGS $ZOOMAIN &quot;$ZOOCFG&quot; &gt; &quot;$_ZOO_DAEMON_OUT&quot; 2&gt;&amp;1 &lt; /dev/null &amp; if [ $? -eq 0 ] then#nohup ----&amp; 这个是标准的后台执行的一个shell脚本的程序#nohup xxx &gt; xxx.log | 2&gt;&amp;1 &amp; #搜索:/ZOO_LOG_DIR= 不存在if [ ! -w &quot;$ZOO_LOG_DIR&quot; ] ; thenmkdir -p &quot;$ZOO_LOG_DIR&quot;fi_ZOO_DAEMON_OUT=&quot;$ZOO_LOG_DIR/zookeeper.out&quot;找到log文件的名字输出是zookeeper.out 3.5 找到log文件的名字输出是zookeeper.out 12345[root@ruozedata001 hadoop]# cat zookeeper.outnohup: failed to run command ‘java’: No such file or directory[root@ruozedata001 hadoop]# [root@ruozedata002 ~]# which java/usr/java/jdk1.8.0_45/bin/java 3.6 分析错误:nohup: failed to run command ‘java’: No such file or directory既然存在，可以先cat start_cluster.sh 可以看到启动的是ssh zuozedata001 …远程执行ssh ruozedata001 “which java” 12345678910[hadoop@ruozedata001 hadoop]$ ssh ruozedata001 &quot;which java&quot;which: no java in (/usr/local/bin:/usr/bin)[hadoop@ruozedata001 hadoop]$ ssh ruozedata001 &quot;echo $JAVA_HOME&quot;/usr/java/jdk1.8.0_45[hadoop@ruozedata001 hadoop]$ ssh ruozedata001 &quot;echo $PATH&quot;/home/hadoop/app/hadoop/bin:/home/hadoop/app/hadoop/sbin:/home/hadoop/app/zookeeper/bin:/usr/java/jdk1.8.0_45/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin[hadoop@ruozedata001 hadoop]$ 只执行which Java[hadoop@ruozedata001 hadoop]$ which java/usr/java/jdk1.8.0_45/bin/java 3.7找到 zkEnv.sh一般写shell脚本时，一般会把公共的配置的脚本抽离出来单独写一个.sh的文件可以定位到zkEnv.sh的一个shell脚本目的是看启动的时候，Java它的赋值在哪 12345if [ -e &quot;$ZOOBIN/../libexec/zkEnv.sh&quot; ]; then . &quot;$ZOOBINDIR/../libexec/zkEnv.sh&quot;else . &quot;$ZOOBINDIR/zkEnv.sh&quot; .表示执行这个shell脚本fi 123456[hadoop@ruozedata001 bin]$ vi zkEnv.shif [ &quot;$JAVA_HOME&quot; != &quot;&quot; ]; then JAVA=&quot;$JAVA_HOME/bin/java&quot;else JAVA=javafi 3.8检查 $JAVA_HOME 123456789echo &quot;--------ruoze: $JAVA_HOME----------&quot;if [ &quot;$JAVA_HOME&quot; != &quot;&quot; ]; then JAVA=&quot;$JAVA_HOME/bin/java&quot;else JAVA=javafi尝试执行启动有打印结果说明JAVA_HOME它的值拿的是空的，那么JAVA赋的值是Java，在shell脚本执行的时候不能找到Java真正执行的命令在哪里 3.8.1 写死路径 12345if [ &quot;$JAVA_HOME&quot; != &quot;&quot; ]; then JAVA=&quot;$JAVA_HOME/bin/java&quot;else JAVA=/usr/java/jdk1.8.0_45/bin/javafi 123456789启动成功[hadoop@ruozedata001 hadoop]$ ssh ruozedata001 &quot;$ZOOKEEPER_HOME/bin/zkServer.sh start&quot;JMX enabled by default-------------------ruoze: ------Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[hadoop@ruozedata001 hadoop]$ ps -ef|grep zookeeperhadoop 5190 1 1 18:32 ? 00:00:00 /usr/java/jdk1.8.0_45/bin/java -Dzookeeper.log.dir=. -Dzookeeper.root.logger=INFO,CONSOLE -cp /home/hadoop/app/zookeeper/bin/../build/classes:/home/hadoop/app/zookeeper/bin/../build/lib/*.jar:/home/hadoop/app/zookeeper/bin/../lib/slf4j-log4j12-1.6.1.jar:/home/hadoop/app/zookeeper/bin/../lib/slf4j-api-1.6.1.jar:/home/hadoop/app/zookeeper/bin/../lib/netty-3.7.0.Final.jar:/home/hadoop/app/zookeeper/bin/../lib/log4j-1.2.16.jar:/home/hadoop/app/zookeeper/bin/../lib/jline-0.9.94.jar:/home/hadoop/app/zookeeper/bin/../zookeeper-3.4.6.jar:/home/hadoop/app/zookeeper/bin/../src/java/lib/*.jar:/home/hadoop/app/zookeeper/bin/../conf: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /home/hadoop/app/zookeeper/bin/../conf/zoo.cfghadoop 5211 2711 0 18:33 pts/0 00:00:00 grep --color=auto zookeeper 3.8.2 ssh执行远程命令和脚本bash模式: 加载环境变量配置文件: 个人.bashrc全局 /etc/profile个人 ~/.bash_profile .bashrc 12345678[hadoop@ruozedata001 hadoop]$ vi ~/.bashrc环境变量配置添加export JAVA_HOME=/usr/java/jdk1.8.0_45export PATH=$JAVA_HOME/bin:$PATH[hadoop@ruozedata001 hadoop]$ ssh ruozedata001 &quot;which java&quot;/usr/java/jdk1.8.0_45/bin/java[hadoop@ruozedata001 hadoop]$ 12345678910启动成功[hadoop@ruozedata001 hadoop]$ kill -9 5190[hadoop@ruozedata001 hadoop]$ ssh ruozedata001 &quot;$ZOOKEEPER_HOME/bin/zkServer.sh start&quot;JMX enabled by default-------------------ruoze: /usr/java/jdk1.8.0_45------Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[hadoop@ruozedata001 hadoop]$ ps -ef|grep zookeeperhadoop 5368 1 2 18:42 ? 00:00:00 /usr/java/jdk1.8.0_45/bin/java -Dzookeeper.log.dir=. -Dzookeeper.root.logger=INFO,CONSOLE -cp /home/hadoop/app/zookeeper/bin/../build/classes:/home/hadoop/app/zookeeper/bin/../build/lib/*.jar:/home/hadoop/app/zookeeper/bin/../lib/slf4j-log4j12-1.6.1.jar:/home/hadoop/app/zookeeper/bin/../lib/slf4j-api-1.6.1.jar:/home/hadoop/app/zookeeper/bin/../lib/netty-3.7.0.Final.jar:/home/hadoop/app/zookeeper/bin/../lib/log4j-1.2.16.jar:/home/hadoop/app/zookeeper/bin/../lib/jline-0.9.94.jar:/home/hadoop/app/zookeeper/bin/../zookeeper-3.4.6.jar:/home/hadoop/app/zookeeper/bin/../src/java/lib/*.jar:/home/hadoop/app/zookeeper/bin/../conf: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /home/hadoop/app/zookeeper/bin/../conf/zoo.cfghadoop 5389 2711 0 18:43 pts/0 00:00:00 grep --color=auto zookeeper 1234567891011121314151617181920[hadoop@ruozedata001 ~]$ scp .bashrc ruozedata002:/home/hadoop/.bashrc 100% 307 0.3KB/s 00:00 [hadoop@ruozedata001 ~]$ scp .bashrc ruozedata003:/home/hadoop/.bashrc 100% 307 0.3KB/s 00:00 [hadoop@ruozedata001 ~]$ cd script/hodoop/[hadoop@ruozedata001 hadoop]$ ./stop_cluster.sh[hadoop@ruozedata001 hadoop]$ ./start_cluster.sh[hadoop@ruozedata001 hadoop]$ ./jps.sh+ echo '----------ruozedata001 process------------'----------ruozedata001 process------------+ ssh ruozedata001 /usr/java/jdk1.8.0_45/bin/jps7249 JobHistoryServer6707 DFSZKFailoverController6899 NodeManager6231 NameNode6088 QuorumPeerMain6793 ResourceManager7322 Jps6334 DataNode6527 JournalNode 全部启动成功","link":"/2020/03/03/shell%E8%84%9A%E6%9C%AC%E5%90%8C%E6%AD%A5%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8Azk%E6%9C%AA%E5%90%AF%E5%8A%A8%E7%9A%84%E6%8E%92%E6%9F%A5%E6%96%B9%E6%B3%95/"},{"title":"Shell命令及脚本编写","text":"一、shell脚本介绍（1）在大数据环境包括所有的环境中，shell脚本通常都是后缀为.sh,有一些会没有后缀（2）shell脚本标准的写法，文件开头的第一行#!/bin/bash这是一个解释器，标识了脚本用什么命令去执行（不标准的写法：没有#!/bin/bash）（3）通常执行脚本的命令是通过 ./xxx.sh ，可以进行绝对路径执行/root/learn_shell/wc.sh或者相对路径./wc.sh（4）或者是使用sh xxx.sh执行脚本，sh是从$PATH环境变量里面找的（5）用./xxx.sh执行脚本需要赋予权限，执行chmod +x xxx.sh或者是chmod u+x xxx.sh，其中+x代表的是用户和用户组都是+x的权限，如果是只想对用户执行权限则u+x 123456789101112131415[root@ruozedata001 learn_shell]# vi wc.sh[root@ruozedata001 learn_shell]# chmod +x wc.sh[root@ruozedata001 learn_shell]# lltotal 4-rwxr-xr-x 1 root root 36 Aug 24 13:00 wc.sh 三组权限都有+x[root@ruozedata001 learn_shell]# chmod -x wc.sh[root@ruozedata001 learn_shell]# chmod u+x wc.sh[root@ruozedata001 learn_shell]# lltotal 4-rwxr--r-- 1 root root 36 Aug 24 13:00 wc.sh 用户有+x权限[root@ruozedata001 learn_shell]# ./wc.shwww.ruozedata.com[root@ruozedata001 learn_shell]# sh -x wc.sh+ echo www.ruozedata.comwww.ruozedata.com 二、shell脚本的使用变量定义与引用、传递参数、数组、if、forwhile、分割、awk、sed 1.变量定义与引用坑：=前后不可出现空格变量名称一般都是大写使用变量 习惯的使用{ } 123456789101112131415161718[root@ruozedata001 learn_shell]# ./variable.shruozedatadateSat Aug 24 13:56:31 CST 2019[root@ruozedata001 learn_shell]# cat variable.sh#!/bin/bashRZ=&quot;ruozedata&quot;DATE1='date'DATE2=`date`echo $rzecho ${date1}echo ${date2}[root@ruozedata001 learn_shell]# 静态k=&quot;v&quot; 'v' v动态k=`v` 2.传递参数 12345678910111213[root@ruozedata001 learn_shell]# ./parameter.sh a bab2a bPID: 2400[root@ruozedata001 learn_shell]# cat parameter.sh#!/bin/bashecho $1echo $2echo &quot;$#&quot; 传递参数的个数echo &quot;$*&quot; 传递参数的列表作为字符串显示，显示所有参数echo &quot;PID: $$&quot; 表示当前脚本运行的PID号 3、数组元素用”空格”符号分割开,数组可以使用[1]来显示数组中元素下标位1的元素，使用[@]来显示所有元素，使用#“数组变量”[@]来显示数组中元素个数 12345678910111213[root@ruozedata001 learn_shell]# vi array.sh[root@ruozedata001 learn_shell]# chmod u+x array.sh[root@ruozedata001 learn_shell]# ./array.shrz jepson xingxing huhuhuhu4[root@ruozedata001 learn_shell]# cat array.sh#!/bin/bash arr=(rz jepson xingxing huhu)echo ${arr[@]}echo ${arr[3]}echo ${#arr[@]}[root@ruozedata001 learn_shell]# 4、if判断需要注意a. [ ] 前后空格b. == 前后空格 12345678910111213[root@ruozedata001 learn_shell]# ./if.shabc=='abc'[root@ruozedata001 learn_shell]# cat if.sh#!/bin/bashA=&quot;abc&quot;B=&quot;jepson&quot;if [ &quot;${A}&quot;== &quot;${B}&quot; ];then echo &quot;${A}==${B}&quot;elif [ &quot;${A}&quot; == &quot;abc&quot; ];then echo &quot;${A}=='abc'&quot;else echo &quot;!=&quot;fi 5、for循环和while循环 12345678910111213141516171819202122232425262728293031323334[root@ruozedata001 learn_shell]# ./forwhile.sh123456789“----------”123456789[root@ruozedata001 learn_shell]# cat forwhile.sh#!/bin/bashfor ((i=1;i&lt;10;i++))do echo $idone echo “----------”j=1while(($j&lt;10)) do echo $j let &quot;j++&quot; done[root@ruozedata001 learn_shell]# 6、分割符 1234567891011121314151617[root@ruozedata001 learn_shell]# chmod u+x spilt.sh[root@ruozedata001 learn_shell]# ./spilt.shrzjxxhh[root@ruozedata001 learn_shell]# cat spilt.sh#!/bin/bashs=&quot;rz,j,xx,hh&quot;OLD_IFS=&quot;$IFS&quot; IFS=&quot;,&quot; arr=($s) IFS=&quot;OLD_IFS&quot; for x in ${arr[*]}do echo $xdone 7、awk 12345678910111213141516171819202122[root@ruozedata001 learn_shell]# cat test.log | awk '{print $1}' 打印文件第一列内容a1[root@ruozedata001 learn_shell]# cat test.log | awk '{print $1,$2}' 打印文件第一列跟第二列内容a b1 2[root@ruozedata001 learn_shell]# cat test.log | awk '{print $1$2}' 打印文件第一列跟第二列内容，无空格ab12[root@ruozedata001 learn_shell]# cat test.log | awk 'NR==1{print}' 打印文件第一行a b c[root@ruozedata001 learn_shell]# cat test.log | awk 'NR==1{print $1}' 打印文件第一行第一列a[root@ruozedata001 learn_shell]# cat test.log | awk 'NR&gt;1{print}' 打印文件大于第一行的内容1 2 3[root@ruozedata001 learn_shell]# cat test.log | awk -F &quot;,&quot; '{print $1}' 以F指定分隔符，打印文件第一列a1[root@ruozedata001 learn_shell]# cat test.log a b c1 2 3[root@ruozedata001 learn_shell]# 8、sedsed 选项 ‘编辑指令’ 文件路径-e：它告诉sed将下一个参数解释为一个sed指令，只有当命令行上给出多个sed指令时才需要使用-e选项i:插入 向匹配行前插入内容d:删除 删除匹配的内容s:替换 替换掉匹配的内容p:打印 打印出匹配的内容，通常与-n选项和用n:读取下一行，遇到n时会自动跳入下一行 1234567891011121314151617[root@ruozedata001 learn_shell]# sed -i 's/a/aa/' test.log sed使用-i的时候，会改变文件内容，不输出信息[root@ruozedata001 learn_shell]# cat test.logaa b c1 2 3[root@ruozedata001 learn_shell]# sed -i &quot;s/a/aa'/&quot; test.log[root@ruozedata001 learn_shell]# cat test.logaa'a b c1 2 3[root@ruozedata001 learn_shell]# sed -i &quot;s/aa'/bbb/&quot; test.log[root@ruozedata001 learn_shell]# cat test.logbbba b c1 2 3[root@ruozedata001 learn_shell]# sed -i &quot;s/b/j/g&quot; test.log sed添加g参数时，才会全局替换，不然只替换文件中每行第一个参数[root@ruozedata001 learn_shell]# cat test.logjjja j c1 2 3[root@ruozedata001 learn_shell]#","link":"/2020/03/03/shell%E5%91%BD%E4%BB%A4%E5%8F%8A%E8%84%9A%E6%9C%AC%E7%BC%96%E5%86%99/"},{"title":"生产上HDFS Block损坏恢复","text":"一.文件ruozedata.md 12345678910111213141516171819202122232425262728293031323334上传:-bash-4.2$ hdfs dfs -mkdir /blockrecover-bash-4.2$ echo &quot;www.ruozedata.com&quot; &gt; ruozedata.md-bash-4.2$ hdfs dfs -put ruozedata.md /blockrecover-bash-4.2$ hdfs dfs -ls /blockrecoverFound 1 items-rw-r--r-- 3 hdfs supergroup 18 2019-03-03 14:42 /blockrecover/ruozedata.md-bash-4.2$ 校验: 健康状态-bash-4.2$ hdfs fsck /Connecting to namenode via http://yws76:50070/fsck?ugi=hdfs&amp;path=%2FFSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 14:44:44 CST 2019...............................................................................Status: HEALTHY Total size: 50194618424 B Total dirs: 354 Total files: 1079 Total symlinks: 0 Total blocks (validated): 992 (avg. block size 50599413 B) Minimally replicated blocks: 992 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 3 Average block replication: 3.0 Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 3 Number of racks: 1FSCK ended at Sun Mar 03 14:44:45 CST 2019 in 76 millisecondsThe filesystem under path '/' is HEALTHY-bash-4.2$ 二.直接DN节点上删除文件一个block的一个副本(3副本) 123456789101112131415161718192021222324252627282930删除块和meta文件:[root@yws87 subdir135]# rm -rf blk_1075808214 blk_1075808214_2068515.meta直接重启HDFS，直接模拟损坏效果，然后fsck检查:-bash-4.2$ hdfs fsck /Connecting to namenode via http://yws77:50070/fsck?ugi=hdfs&amp;path=%2FFSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 16:02:04 CST 2019./blockrecover/ruozedata.md: Under replicated BP-1513979236-192.168.0.76-1514982530341:blk_1075808214_2068515. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s)................................................................................Status: HEALTHY Total size: 50194618424 B Total dirs: 354 Total files: 1079 Total symlinks: 0 Total blocks (validated): 992 (avg. block size 50599413 B) Minimally replicated blocks: 992 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 1 (0.10080645 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 3 Average block replication: 2.998992 Corrupt blocks: 0 Missing replicas: 1 (0.033602152 %) Number of data-nodes: 3 Number of racks: 1FSCK ended at Sun Mar 03 16:02:04 CST 2019 in 148 millisecondsThe filesystem under path '/' is HEALTHY-bash-4.2$ 三.手动修复hdfs debug 1234567891011121314151617181920-bash-4.2$ hdfs |grep debug没有输出debug参数的任何信息结果！故hdfs命令帮助是没有debug的，但是确实有hdfs debug这个组合命令，切记。修复命令:-bash-4.2$ hdfs debug recoverLease -path /blockrecover/ruozedata.md -retries 10recoverLease SUCCEEDED on /blockrecover/ruozedata.md-bash-4.2$ 直接DN节点查看，block文件和meta文件恢复:[root@yws87 subdir135]# lltotal 8-rw-r--r-- 1 hdfs hdfs 56 Mar 3 14:28 blk_1075808202-rw-r--r-- 1 hdfs hdfs 11 Mar 3 14:28 blk_1075808202_2068503.meta[root@yws87 subdir135]# lltotal 24-rw-r--r-- 1 hdfs hdfs 56 Mar 3 14:28 blk_1075808202-rw-r--r-- 1 hdfs hdfs 11 Mar 3 14:28 blk_1075808202_2068503.meta-rw-r--r-- 1 hdfs hdfs 18 Mar 3 15:23 blk_1075808214-rw-r--r-- 1 hdfs hdfs 11 Mar 3 15:23 blk_1075808214_2068515.meta 四.自动修复 123456789当数据块损坏后，DN节点执行directoryscan操作之前，都不会发现损坏；也就是directoryscan操作是间隔6hdfs.datanode.directoryscan.interval : 21600在DN向NN进行blockreport前，都不会恢复数据块;也就是blockreport操作是间隔6hdfs.blockreport.intervalMsec : 21600000当NN收到blockreport才会进行恢复操作。 总结: 生产上本人一般倾向于使用 手动修复方式，但是前提要手动删除损坏的block块。切记，是删除损坏block文件和meta文件，而不是删除hdfs文件。 当然还可以先把文件get下载，然后hdfs删除，再对应上传。 切记删除不要执行: hdfs fsck / -delete 这是删除损坏的文件， 那么数据不就丢了嘛；除非无所谓丢数据，或者有信心从其他地方可以补数据到hdfs！","link":"/2020/03/03/%E7%94%9F%E4%BA%A7%E4%B8%8AHDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D/"},{"title":"使用VMware 安装CentOS6.9系统","text":"CentOS安装简略步骤第一 自定义配置并选择虚拟机的硬件兼容性(workstation10.0)第二选择创建虚拟机空白磁盘，并选择安装的系统Linux和版本CentOS 64位 第三 选择虚拟机使用的内存，根据电脑的配置来选择，内存尽可能大一些，最低为2G第四 网络类型选择为 使用网络地址转换（NAT）第五 点击下一步至指定磁盘容量和磁盘大小环节，设置最大磁盘大小为50G,选择将虚拟机拆分成多个文件，最后指定磁盘位置第六 进入虚拟机右键编辑虚拟机设置第七 进入到安装类型的时候选择自定义安装方式))第八选择现在安装至安装完成第九 开始配置网络环境，先把电脑和虚拟机的防火墙关闭 并开启VMware两个服务)第十配置无线网卡的网络，查看电脑IP的方式是cmd命令运行 ipconfig -all 命令查看 DNS 、IPv4第十一 进入VM 编辑-虚拟网络编辑器 进行NAT和DHCP设置))))","link":"/2020/03/02/%E5%AE%89%E8%A3%85CentOS6.9%E7%B3%BB%E7%BB%9F/"},{"title":"进程PID与端口号PORT","text":"一个进程有很多个端口号（对内），只有一个端口号（对外） 1、关于进程pid进程管理会用到的命令|命令| 含义 ||–|–|| ps -ef |查看系统中的进程 || grep | 将后台进程中指定的文件过滤出来 ||grep -v|过滤排除|| kill -9 pid | 杀死进程 ||kill -9 $(pgrep -f tail)| 杀死关于tail命令的所有进程 | kill -9 pid杀死多个进程：kill -9 2873 3764pgrep -f tail 将tail过滤出来的进程打印出来[root@hadoop001 ~]# echo $(pgrep -f tail)将打印出来的进程由纵列变成横列[root@hadoop001 ~]# kill -9 $(pgrep -f tail)杀死关于tail命令的所有进程提醒：生产上假如非要执行kill杀进程，一定要确认清楚是否真的需要kill 查看进程执行如下： 12345678910[root@hadoop001 ~]# ps -ef|grep tailroot 3140 2981 0 02:05 pts/1 00:00:00 tail -F tail.logroot 3144 2919 0 02:06 pts/0 00:00:00 grep tail[root@hadoop001 ~]# ps -ef|grep tail | grep -v greproot 3140 2981 0 02:05 pts/1 00:00:00 tail -F tail.log[root@hadoop001 ~]# ps -ef|grep 2981root 2981 2977 0 01:28 pts/1 00:00:00 -bashroot 3140 2981 0 02:05 pts/1 00:00:00 tail -F tail.logroot 3172 2919 0 02:19 pts/0 00:00:00 grep 2981[root@hadoop001 ~]# 2、关于端口号port查找端口号是先通过进程名称找到pid ，再通过pid找到该进程的端口号以ssh为例 123456789[root@hadoop001 ~]# ps -ef|grep sshroot 1475 1 0 Jun19 ? 00:00:00 /usr/sbin/sshdroot 2915 1475 0 01:09 ? 00:00:01 sshd: root@pts/0 root 2977 1475 0 01:28 ? 00:00:00 sshd: root@pts/1 root 3230 2919 0 02:42 pts/0 00:00:00 grep ssh[root@hadoop001 ~]# netstat -nlp|grep 1475tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1475/sshd tcp 0 0 :::22 :::* LISTEN 1475/sshd [root@hadoop001 ~]# ssh服务的端口是22，一般ssh端口号不能为22，需要调整，防止攻击端口号对外服务的ip地址，假如为127.0.0.1或localhost,只能在这台的机器上访问这个服务。一般这个地址为机器的ip或0.0.0.0 或 :::，表示对外的任意ip可以服务过滤进程的正常流程：进程名称–&gt;pid–&gt;port可以通过port检验端口号是否正确是否变更","link":"/2020/03/02/%E8%BF%9B%E7%A8%8BPID%E4%B8%8E%E7%AB%AF%E5%8F%A3%E5%8F%B7PORT/"},{"title":"登录MySQL时出现Access denied的错误提示","text":"适用于忘记密码时，跳过权限认证一.安装好mysql后，登录时提示Access denied 123hadoop001:mysqladmin:/usr/local/mysql:&gt;mysql -uroot -pEnter password: ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO) 二.查看配置文件，将skip-grant-tables放在[mysqld]下面skip-grant-tables的作用是跳过权限认证skip-external-locking的作用是跳过外部锁定 12345[mysqld]port = 3306socket = /usr/local/mysql/data/mysql.sockskip-grant-tablesskip-external-locking 三.重新启动服务或者关闭使用的crt、xshell 1234hadoop001:mysqladmin:/usr/local/mysql:&gt;service mysql stopShutting down MySQL.. [ OK ]hadoop001:mysqladmin:/usr/local/mysql:&gt;service mysql startShutting MySQL.. [ OK ] 四.重新登录MySQL 123456789101112131415hadoop001:mysqladmin:/usr/local/mysql:&gt;mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 1Server version: 5.6.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt;","link":"/2020/03/03/%E7%99%BB%E5%BD%95MySQL%E6%97%B6%E5%87%BA%E7%8E%B0Access%20denied%E7%9A%84%E9%94%99%E8%AF%AF%E6%8F%90%E7%A4%BA/"},{"title":"配置hadoop组件yarn及案例测试","text":"一、vi命令模式编辑配置mapred-site.xml参数配置etc/hadoop/yarn-site.xml参数 12345678910111213[hadoop@hadoop001 etc]$ cd hadoop[hadoop@hadoop001 hadoop]$ ll#-rw-r--r--. 1 hadoop hadoop 758 Mar 24 2016 mapred-site.xml.template[hadoop@hadoop001 hadoop]$ cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop001 hadoop]$ vi mapred-site.xml[hadoop@hadoop001 hadoop]$ cat mapred-site.xml#&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;#&lt;/configuration&gt; 1234567[hadoop@hadoop001 hadoop]$ vi yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 二、启动yarn服务 1234567891011121314#ResourceManager rm 资源管理者#NodeManager nm 节点管理者[hadoop@hadoop001 hadoop]$ cd ../../[hadoop@hadoop001 hadoop]$ sbin/start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /home/hadoop/software/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop001.outhadoop001: starting nodemanager, logging to /home/hadoop/software/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop001.out[hadoop@hadoop001 hadoop]$ jps5296 SecondaryNameNode5010 NameNode5107 DataNode5940 NodeManager5848 ResourceManager6251 Jps 三、验证 12345678[hadoop@hadoop001 hadoop]$ netstat -nlp|grep 5848(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)tcp 0 0 :::8088 :::* LISTEN 5848/java tcp 0 0 :::8030 :::* LISTEN 5848/java tcp 0 0 :::8031 :::* LISTEN 5848/java tcp 0 0 :::8032 :::* LISTEN 5848/java tcp 0 0 :::8033 :::* LISTEN 5848/java 四、案例展示 1234567891011121314151617181920212223242526272829303132333435363738394041[hadoop@hadoop001 hadoop]$ find ./ -name '*example*.jar'./share/hadoop/mapreduce1/hadoop-examples-2.6.0-mr1-cdh5.7.0.jar./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.7.0-test-sources.jar./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.7.0-sources.jar./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar[hadoop@hadoop001 hadoop]$ [hadoop@hadoop001 hadoop]$ bin/hadoop jar ./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar[hadoop@hadoop001 hadoop]$ bin/hadoop jar ./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcountUsage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;造一组词[hadoop@hadoop001 hadoop]$ vi 1.log [hadoop@hadoop001 hadoop]$ vi 2.log[hadoop@hadoop001 hadoop]$ bin/hdfs dfs -mkdir /examples/input19/07/06 18:54:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable[hadoop@hadoop001 hadoop]$ bin/hdfs dfs -put *.log /examples/input19/07/06 18:56:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable#查看vi命令编辑的1.log和2.log是否存在[hadoop@hadoop001 hadoop]$ bin/hdfs dfs -ls /examples/input 19/07/06 18:57:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 items-rw-r--r-- 1 hadoop supergroup 16 2019-07-06 18:56 /examples/input/1.log-rw-r--r-- 1 hadoop supergroup 38 2019-07-06 18:56 /examples/input/2.log#执行jar包wordcount的输入输出[hadoop@hadoop001 hadoop]$ bin/hadoop jar \\./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar \\wordcount /examples/input /examples/output1.....[hadoop@hadoop001 hadoop]$ bin/hdfs dfs -ls /examples/output119/07/03 22:49:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 items-rw-r--r-- 1 hadoop supergroup 0 2019-07-03 22:40 /examples/output1/_SUCCESS-rw-r--r-- 1 hadoop supergroup 44 2019-07-03 22:40 /examples/output1/part-r-00000#结果输出[hadoop@hadoop001 hadoop]$ bin/hdfs dfs -ls /examples/output119/07/03 22:49:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 items-rw-r--r-- 1 hadoop supergroup 0 2019-07-03 22:40 /examples/output1/_SUCCESS-rw-r--r-- 1 hadoop supergroup 44 2019-07-03 22:40 /examples/output1/part-r-00000","link":"/2020/03/03/%E9%85%8D%E7%BD%AEhadoop%E7%BB%84%E4%BB%B6yarn%E5%8F%8A%E6%A1%88%E4%BE%8B%E6%B5%8B%E8%AF%95/"},{"title":"配置多台机器SSH相互通信信任","text":"1.5台机器执行 ssh-keygen[root@sht-sgmhadoopnn-01 ~]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Created directory ‘/root/.ssh’.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:1c:68:d2:13:01:e5:f0:36:30:bb:1a:72:09:6d:e1:45 root@sht-sgmhadoopnn-01.telenav.cnThe key’s randomart image is:+–[ RSA 2048]—-+| ..Eo+. || o o O o ||. + o X . || o . = + . ||. + . S || o o || . || || |+—————–+ 2.选取第一台,生成authorized_keys文件[root@sht-sgmhadoopnn-01 ~]# cd .ssh[root@sht-sgmhadoopnn-01 .ssh]# cat /root/.ssh/id_rsa.pub&gt;&gt; /root/.ssh/authorized_keys 3.然后将其他四台的id_rsa.pub内容,手动copy到第一台的authorized_keys文件[root@sht-sgmhadoopnn-02 .ssh]# more id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAxdcOFyF90ioQzC8OCEZ5dIFgqiJ2G7MGmC4LSwl5cVs1J1E9HQesEURjjPdQGJP1kIe8Z2NgCpjGy7BDiVFvj/0fUjGreRN9P2LPa1jUv0xBYHv9wk+aN2YtKy2Dc9WeCPaNbByyz0n96osE0NVict+2MGQJHdHFedG2sSuTdBAoXE1I7ag6AwWV/3ije26BD88aZmb/Z9c0fqwei57l0kdXRPsxzGIWCL4rCT1Vwu8VYTNLhA+QzTJfJq5GCbo1A5PFuuE+vXrIEloKp1Y6Y1xB8xxdgqvMAkvf7wsWfZVFl6vZPwRJivIKW6WPm7tlJN2m3xXyXnXy0/8rftB0Aw== root@sht-sgmhadoopnn-02.telenav.cn[root@sht-sgmhadoopdn-01 .ssh]# more id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAre6qKMotVwy94ZPBtwsvW9dM9yVar6MGW3QIxuAgcRbfqE4RRrsgcnwFBG496+GPLUjfQsYuPhuLm+2qu8p2pTkLx9Vt0ppK+VlNwQIQyias2hiHoLDGmcstCeCZs+sn5iZ2/rTov0uOZT7XWC66QlnFzAyM80KYAFLtFv4r9uU+KK5USwEG3XwF1GiAeSK34iU9u+JIks6/b0zXhT2lxgsj/N4i4Ze/xqkGLCNfkbZEVEivKknxM9A69Hg031ItxVfyA0k/M3kFI6VmXTNgkf8VW/[root@sht-sgmhadoopdn-02 .ssh]# more id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAvvkGCZm+/g2nfQ0zDXGp7mwo0s5JIdcYlVu0A0a2uDru3VaYFZ5uD79hLQJqPk1qT49wAFmXfIoUGtyHvDYfYcGcRVwQV5c2EnLmb0gxBL45hwHoVGyHw7EOAW5XUxZvkf6gLua/N8htmfd5O8dhhtAhIK3LB8z0mOTeUmORX1AEFT74huGrrp7fzY+kIr5KIjeU5dNCK+VbSwTVicyNkQN6OM3RVPGSm33niLh+uKBxjrpKtyhN0MxC+EIo+osVXcvb4Zox3QrFmMklDZf/pyRKBqQW2yBTi5U16hO1/TXxMYamz48Ps2fGx2fDvDAB4RsSwYaQ0fSM2ghd6oRiCQ== root@sht-sgmhadoopdn-02.telenav.cn[root@sht-sgmhadoopdn-03 .ssh]# more id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAuMagBMNiFqKN9xp94V5QACVnN5V+eV0CsL2evA+0qjwCTFSCFuCM6TgC7anurx9tzjT7P9G8jogy76EJn8MXZVJ7Dfou+hsOK7flGt24DHqqoIDwKOKsA9XsRSUGz7T78EQhGLhPD4Ud1C8WEQEgSL11ocl8fNnMlYzJuQFndV6FbKYb9GJx5rI6nlZ6KS1pUVMkq/TG+tuzLNgQTx5Ed0j6LxR431QJiTWWgRdNd2C7U/RkV9D8eguUOZCdNT++ED275gwvx1hRQzsK4h9q90XOgWG1+ol/V13toqo7HudOAqJNnWGznU9O30zp4WAdhuWCcWGtK8dhnWiw0bRNKw== root@sht-sgmhadoopdn-03.telenav.cn 拷贝至authorized_keys文件(注意copy时,最好先放到记事本中,将回车去掉,成为一行)[root@sht-sgmhadoopnn-01 .ssh]# vi authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA4TLoppFAf0IgGQFAEwHWOKKtjMjk093LLeQaCyndvu1qNd+FAHFUhBtC37zgD7xVR4uXrBNZxt3NEQnKurFyN3sJ0I7VRi+vX/X3FWzJfIAwCeFFu5pk2jrosQijqcY92t5FLLINRPLL3qI/t4tVxk2+PwRF6GuDgBE0IX++snHngpHA2Tr8DB8otE6eJlUSg+dsRqhlC4teC6PC0vfjWRS7O/dgv8+sIU7Y4RAJR2KQJiXoliBELeCOvkeWUaV66NE1Qe2VGLyBYYqJQ5PSl2jhH4Lsj+p70H0Cuyni1IHg/xKjKuaRm3WVJrFiS58dDg43SVpI+UQ4iYbgpB0dhw== root@sht-sgmhadoopnn-01.telenav.cnssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAxdcOFyF90ioQzC8OCEZ5dIFgqiJ2G7MGmC4LSwl5cVs1J1E9HQesEURjjPdQGJP1kIe8Z2NgCpjGy7BDiVFvj/0fUjGreRN9P2LPa1jUv0xBYHv9wk+aN2YtKy2Dc9WeCPaNbByyz0n96osE0NVict+2MGQJHdHFedG2sSuTdBAoXE1I7ag6AwWV/3ije26BD88aZmb/Z9c0fqwei57l0kdXRPsxzGIWCL4rCT1Vwu8VYTNLhA+QzTJfJq5GCbo1A5PFuuE+vXrIEloKp1Y6Y1xB8xxdgqvMAkvf7wsWfZVFl6vZPwRJivIKW6WPm7tlJN2m3xXyXnXy0/8rftB0Aw== root@sht-sgmhadoopnn-02.telenav.cnssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAre6qKMotVwy94ZPBtwsvW9dM9yVar6MGW3QIxuAgcRbfqE4RRrsgcnwFBG496+GPLUjfQsYuPhuLm+2qu8p2pTkLx9Vt0ppK+VlNwQIQyias2hiHoLDGmcstCeCZs+sn5iZ2/rTov0uOZT7XWC66QlnFzAyM80KYAFLtFv4r9uU+KK5USwEG3XwF1GiAeSK34iU9u+JIks6/b0zXhT2lxgsj/N4i4Ze/xqkGLCNfkbZEVEivKknxM9A69Hg031ItxVfyA0k/M3kFI6VmXTNgkf8VW/uHl92xiRfTn1C065iiE7vFqSkcsnCr6hxwFB3nNDTZYGx6GYsdeGlrWi2rdQ== root@sht-sgmhadoopdn-01.telenav.cnssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAvvkGCZm+/g2nfQ0zDXGp7mwo0s5JIdcYlVu0A0a2uDru3VaYFZ5uD79hLQJqPk1qT49wAFmXfIoUGtyHvDYfYcGcRVwQV5c2EnLmb0gxBL45hwHoVGyHw7EOAW5XUxZvkf6gLua/N8htmfd5O8dhhtAhIK3LB8z0mOTeUmORX1AEFT74huGrrp7fzY+kIr5KIjeU5dNCK+VbSwTVicyNkQN6OM3RVPGSm33niLh+uKBxjrpKtyhN0MxC+EIo+osVXcvb4Zox3QrFmMklDZf/pyRKBqQW2yBTi5U16hO1/TXxMYamz48Ps2fGx2fDvDAB4RsSwYaQ0fSM2ghd6oRiCQ== root@sht-sgmhadoopdn-02.telenav.cnssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAuMagBMNiFqKN9xp94V5QACVnN5V+eV0CsL2evA+0qjwCTFSCFuCM6TgC7anurx9tzjT7P9G8jogy76EJn8MXZVJ7Dfou+hsOK7flGt24DHqqoIDwKOKsA9XsRSUGz7T78EQhGLhPD4Ud1C8WEQEgSL11ocl8fNnMlYzJuQFndV6FbKYb9GJx5rI6nlZ6KS1pUVMkq/TG+tuzLNgQTx5Ed0j6LxR431QJiTWWgRdNd2C7U/RkV9D8eguUOZCdNT++ED275gwvx1hRQzsK4h9q90XOgWG1+ol/V13toqo7HudOAqJNnWGznU9O30zp4WAdhuWCcWGtK8dhnWiw0bRNKw== root@sht-sgmhadoopdn-03.telenav.cn 4.权限(每台机器)chmod 700 -R ~/.sshchmod 600 ~/.ssh/authorized_keys 5.将第一台的authorized_keys scp 给其他四台(第一次传输,需要输入密码)[root@sht-sgmhadoopnn-01 .ssh]# scp authorized_keys root@sht-sgmhadoopnn-02:/root/.sshroot@sht-sgmhadoopnn-02’s password:authorized_keys 100% 2080 2.0KB/s 00:00[root@sht-sgmhadoopnn-01 .ssh]# scp authorized_keys root@sht-sgmhadoopdn-01:/root/.sshroot@sht-sgmhadoopdn-01’s password:authorized_keys 100% 2080 2.0KB/s 00:00[root@sht-sgmhadoopnn-01 .ssh]# scp authorized_keys root@sht-sgmhadoopdn-02:/root/.sshroot@sht-sgmhadoopdn-02’s password:authorized_keys 100% 2080 2.0KB/s 00:00[root@sht-sgmhadoopnn-01 .ssh]# scp authorized_keys root@sht-sgmhadoopdn-03:/root/.sshroot@sht-sgmhadoopdn-03’s password:authorized_keys 100% 2080 2.0KB/s 00:00 6.验证(每台机器上执行下面5条命令,只输入yes,不输入密码,则这5台互相通信了)ssh root@sht-sgmhadoopnn-01 datessh root@sht-sgmhadoopnn-02 datessh root@sht-sgmhadoopdn-01 datessh root@sht-sgmhadoopdn-02 datessh root@sht-sgmhadoopdn-03 date","link":"/2020/03/03/%E9%85%8D%E7%BD%AE%E5%A4%9A%E5%8F%B0%E6%9C%BA%E5%99%A8SSH%E7%9B%B8%E4%BA%92%E9%80%9A%E4%BF%A1%E4%BF%A1%E4%BB%BB/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"Hive","slug":"Hive","link":"/tags/Hive/"},{"name":"部署","slug":"部署","link":"/tags/%E9%83%A8%E7%BD%B2/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"MapReduce","slug":"MapReduce","link":"/tags/MapReduce/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"centOS","slug":"centOS","link":"/tags/centOS/"},{"name":"tomcat","slug":"tomcat","link":"/tags/tomcat/"},{"name":"Shell","slug":"Shell","link":"/tags/Shell/"},{"name":"虚拟机","slug":"虚拟机","link":"/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}],"categories":[{"name":"Hadoop","slug":"Hadoop","link":"/categories/Hadoop/"},{"name":"Hive","slug":"Hive","link":"/categories/Hive/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"MapReduce","slug":"MapReduce","link":"/categories/MapReduce/"},{"name":"Spark","slug":"Spark","link":"/categories/Spark/"},{"name":"云服务器","slug":"云服务器","link":"/categories/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"Shell","slug":"Shell","link":"/categories/Shell/"},{"name":"虚拟机","slug":"虚拟机","link":"/categories/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}]}